 computer performance is characterized by the amount of useful work accomplished by a computer system or computer network compared to the time and resources used  depending on the context  high computer performance may involve one or more of the following             the performance of any computer system can be evaluated in measurable  technical terms  using one or more of the metrics listed above  this way the performance can be    whilst the above definition relates to a scientific  technical approach  the following definition given by arnold allen would be useful for a non technical audience     the word performance in computer performance means the same thing that performance means in other contexts  that is  it means  how well is the computer doing the work it is supposed to do       computer software performance  particularly software application response time  is an aspect of software quality that is important in human computer interactions     performance engineering within systems engineering  encompasses the set of roles  skills  activities  practices  tools  and deliverables applied at every phase of the systems development life cycle which ensures that a solution will be designed  implemented  and operationally supported to meet the performance requirements defined for the solution     performance engineering continuously deals with trade offs between types of performance  occasionally a cpu designer can find a way to make a cpu with better overall performance by improving one of the aspects of performance  presented below  without sacrificing the cpu s performance in other areas  for example  building the cpu out of better  faster transistors     however  sometimes pushing one type of performance to an extreme leads to a cpu with worse overall performance  because other important aspects were sacrificed to get one impressive looking number  for example  the chip s clock rate  see the megahertz myth      application performance engineering  ape  is a specific methodology within performance engineering designed to meet the challenges associated with application performance in increasingly distributed mobile  cloud and terrestrial it environments  it includes the roles  skills  activities  practices  tools and deliverables applied at every phase of the application lifecycle that ensure an application will be designed  implemented and operationally supported to meet non functional performance requirements     computer performance metrics  things to measure  include availability  response time  channel capacity  latency  completion time  service time  bandwidth  throughput  relative efficiency  scalability  performance per watt  compression ratio  instruction path length and speed up  cpu benchmarks are available      availability of a system is typically measured as a factor of its reliability   as reliability increases  so does availability  that is  less downtime   availability of a system may also be increased by the strategy of focusing on increasing testability and maintainability and not on reliability  improving maintainability is generally easier than reliability  maintainability estimates  repair rates  are also generally more accurate  however  because the uncertainties in the reliability estimates are in most cases very large  it is likely to dominate the availability  prediction uncertainty  problem  even while maintainability levels are very high     response time is the total amount of time it takes to respond to a request for service  in computing  that service can be any unit of work from a simple disk io to loading a complex web page  the response time is the sum of three numbers      most consumers pick a computer architecture  normally intel ia   architecture  to be able to run a large base of pre existing  pre compiled software  being relatively uninformed on computer benchmarks  some of them pick a particular cpu based on operating frequency  see megahertz myth      some system designers building parallel computers pick cpus based on the speed per dollar     channel capacity is the tightest upper bound on the rate of information that can be reliably transmitted over a communications channel  by the noisy channel coding theorem  the channel capacity of a given channel is the limiting information rate  in units of information per unit time  that can be achieved with arbitrarily small error probability       information theory  developed by claude e  shannon during world war ii  defines the notion of channel capacity and provides a mathematical model by which one can compute it  the key result states that the capacity of the channel  as defined above  is given by the maximum of the mutual information between the input and output of the channel  where the maximization is with respect to the input distribution      latency is a time delay between the cause and the effect of some physical change in the system being observed  latency is a result of the limited velocity with which any physical interaction can take place  this velocity is always lower or equal to speed of light  therefore every physical system that has spatial dimensions different from zero will experience some sort of latency     the precise definition of latency depends on the system being observed and the nature of stimulation  in communications  the lower limit of latency is determined by the medium being used for communications  in reliable two way communication systems  latency limits the maximum rate that information can be transmitted  as there is often a limit on the amount of information that is  in flight  at any one moment  in the field of human machine interaction  perceptible latency  delay between what the user commands and when the computer provides the results  has a strong effect on user satisfaction and usability     computers run sets of instructions called a process  in operating systems  the execution of the process can be postponed if other processes are also executing  in addition  the operating system can schedule when to perform the action that the process is commanding  for example  suppose a process commands that a computer card s voltage output be set high low high low and so on at a rate of     xa hz  the operating system may choose to adjust the scheduling of each transition  high low or low high  based on an internal clock  the latency is the delay between the process instruction commanding the transition and the hardware actually transitioning the voltage from high to low or low to high     system designers building real time computing systems want to guarantee worst case response  that is easier to do when the cpu has low interrupt latency and when it has deterministic response     in computer networking  bandwidth is a measurement of bit rate of available or consumed data communication resources  expressed in bits per second or multiples of it  bit s  kbit s  mbit s  gbit s  etc       bandwidth sometimes defines the net bit rate  aka  peak bit rate  information rate  or physical layer useful bit rate   channel capacity  or the maximum throughput of a logical or physical communication path in a digital communication system  for example  bandwidth tests measure the maximum throughput of a computer network  the reason for this usage is that according to hartley s law  the maximum data rate of a physical communication link is proportional to its bandwidth in hertz  which is sometimes called frequency bandwidth  spectral bandwidth  rf bandwidth  signal bandwidth or analog bandwidth     in general terms  throughput is the rate of production or the rate at which something can be processed     in communication networks  throughput is essentially synonymous to digital bandwidth consumption  in wireless networks or cellular systems  the system spectral efficiency in bit s hz area unit  bit s hz site or bit s hz cell  is the maximum system throughput  aggregate throughput  divided by the analog bandwidth and some measure of the system coverage area     in integrated circuits  often a block in a data flow diagram has a single input and a single output  and operate on discrete packets of information  examples of such blocks are fft modules or binary multipliers  because the units of throughput are the reciprocal of the unit for propagation delay  which is  seconds per message  or  seconds per output   throughput can be used to relate a computational device performing a dedicated function such as an asic or embedded processor to a communications channel  simplifying system analysis     scalability is the ability of a system  network  or process to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth    the amount of electricity used by the computer  this becomes especially important for systems with limited power sources such as solar  batteries  human power     system designers building parallel computers  such as google s hardware  pick cpus based on their speed per watt of power  because the cost of powering the cpu outweighs the cost of the cpu itself      compression is useful because it helps reduce resource usage  such as data storage space or transmission capacity  because compressed data must be decompressed to use  this extra processing imposes computational or other costs through decompression  this situation is far from being a free lunch  data compression is subject to a space time complexity trade off     this is an important performance feature of mobile systems  from the smart phones you keep in your pocket to the portable embedded systems in a spacecraft     the effect of a computer or computers on the environment  during manufacturing and recycling as well as during use  measurements are taken with the objectives of reducing waste  reducing hazardous materials  and minimizing a computer s ecological footprint     because there are so many programs to test a cpu on all aspects of performance  benchmarks were developed     the most famous benchmarks are the specint and specfp benchmarks developed by standard performance evaluation corporation and the consumermark benchmark developed by the embedded microprocessor benchmark consortium eembc     in software engineering  performance testing is in general testing performed to determine how a system performs in terms of responsiveness and stability under a particular workload  it can also serve to investigate  measure  validate or verify other quality attributes of the system  such as scalability  reliability and resource usage     performance testing is a subset of performance engineering  an emerging computer science practice which strives to build performance into the implementation  design and architecture of a system     in software engineering  profiling   program profiling    software profiling   is a form of dynamic program analysis that measures  for example  the space  memory  or time complexity of a program  the usage of particular instructions  or frequency and duration of function calls  the most common use of profiling information is to aid program optimization     profiling is achieved by instrumenting either the program source code or its binary executable form using a tool called a profiler  or code profiler   a number of different techniques may be used by profilers  such as event based  statistical  instrumented  and simulation methods     performance tuning is the improvement of system performance  this is typically a computer application  but the same methods can be applied to economic markets  bureaucracies or other complex systems  the motivation for such activity is called a performance problem  which can be real or anticipated  most systems will respond to increased load with some degree of decreasing performance  a system s ability to accept a higher load is called scalability  and modifying a system to handle a higher load is synonymous to performance tuning     systematic tuning follows these steps     perceived performance  in computer engineering  refers to how quickly a software feature appears to perform its task  the concept applies mainly to user acceptance aspects     the amount of time an application takes to start up  or a file to download  is not made faster by showing a startup screen  see splash screen  or a file progress dialog box  however  it satisfies some human needs  it appears faster to the user as well as providing a visual cue to let them know the system is handling their request     in most cases  increasing real performance increases perceived performance  but when real performance cannot be increased due to physical limitations  techniques can be used to increase perceived performance at the cost of marginally decreasing real performance         the total amount of time  t  required to execute a particular benchmark program is    where    even on one machine  a different compiler or the same compiler with different compiler optimization switches can change n and cpi the benchmark executes faster if the new compiler can improve n or c without making the other worse  but often there is a trade off between them is it better  for example  to use a few complicated instructions that take a long time to execute  or to use instructions that execute very quickly  although it takes more of them to execute the benchmark      a cpu designer is often required to implement a particular instruction set  and so cannot change n  sometimes a designer focuses on improving performance by making significant improvements in f  with techniques such as deeper pipelines and faster caches   while  hopefully  not sacrificing too much c leading to a speed demon cpu design  sometimes a designer focuses on improving performance by making significant improvements in cpi  with techniques such as out of order execution  superscalar cpus  larger caches  caches with improved hit rates  improved branch prediction  speculative execution  etc    while  hopefully  not sacrificing too much clock frequency leading to a brainiac cpu design   for a given instruction set  and therefore fixed n  and semiconductor process  the maximum single thread performance    t  requires a balance between brainiac techniques and speedracer techniques   