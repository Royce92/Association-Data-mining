 distributed file system for cloud is a file system that allows many clients to have access to the same data file providing important operations  create  delete  modify  read  write   each file may be partitioned into several parts called chunks  each chunk is stored in remote machines typically  data is stored in files in a hierarchical tree where the nodes represent the directories  hence  it facilitates the parallel execution of applications  there are several ways to share files in a distributed architecture  each solution must be suitable for a certain type of application relying on how complex is the application or how simple it is  meanwhile  the security of the system must be ensured  confidentiality  availability and integrity are the main keys for a secure system  nowadays  users can share resources from any computer device  anywhere and everywhere through internet thanks to cloud computing which is typically characterized by the scalable and elastic resources  such as physical servers  applications and any services that are virtualized and allocated dynamically  thus  synchronization is required to make sure that all devices are update  distributed file systems enable also many big  medium and small enterprises to store and access their remote data exactly as they do locally  facilitating the use of variable resources             today  there are many implementations of distributed file systems  the first file servers were developed by researchers in the     s  and the sun s network file system were disposable in the early       before that  people who wanted to share files used the sneakernet method  once the computer networks start to progress  it became obvious that the existing file systems had a lot of limitations and were unsuitable for multi user environments  at the beginning  many users started to use ftp to share files   it started running on the pdp    in the end of       even with ftp  files needed to be copied from the source computer onto a server and also from the server onto the destination computer  and that force the users to know the physical addresses of all computers concerned by the file sharing      cloud computing use important techniques to enforce the performance of all the system  modern data centers provide a huge environment with data center networking  dcn  and consisting of big number of computers characterized by different capacity of storage  mapreduce framework had shown its performance with data intensive computing applications in a parallel and distributed system  moreover  virtualization technique has been employed to provide dynamic resource allocation and allowing multiple operating systems to coexist on the same physical server     as cloud computing provides a large scale computing thanks to its ability of providing to the user the needful cpu and storage resources with a complete transparency  it makes it very suitable to different types of applications that require a large scale distributed processing  that kind of data intensive computing needs a high performance file system that can share data between vms  virtual machine       the application of the cloud computing and cluster computing paradigms are becoming increasingly important in the industrial data processing and scientific applications such as astronomy or physic ones that frequently demand the availability of a huge number of computers in order to lead the required experiments  the cloud computing have represent a new way of using the computing infrastructure by dynamically allocating the needed resources  release them once it is finished and only pay for what they use instead of paying some resources  for a certain time fixed earlier the pas as you go model   that kind of services is often provide in the context of service level agreement      most of distributed file systems are built on the client server architecture  but yet others decentralized solutions exist as well     nfs is the one of the most that use this architecture  it enables to share files between a certain number of machines on a network as if they were located locally  it provides a standardized view of the local file system  the nfs protocol allows heterogeneous clients  process   probably running on different operating systems and machines  to access the files on a distant server  ignoring the actual location of files  however  relying on a single server makes the nfs protocol suffering form a low availability and a poor scalability  using multiple servers does not solve the problem since each server is working independently   the model of nfs is the remote file service  this model is also called the remote access model which is in contrast with the upload download model     the file system offered by nfs is almost the same as the one offered by unix systems  files are hierarchically organized into a naming graph in which directories and files are represented by nodes     it is rather an amelioration of client server architecture in a way that improve the execution of parallel application  the technique used here is the file striping one  this technique lead to split a file into several segments in order to save them in multiple servers  the goal is to have access to different parts of a file in parallel  if the application does not benefit from this technique  then it could be more convenient to just store different files on different servers  however  when it comes to organize a distributed file system for large data centers such as amazon and google that offer services to web clients allowing multiple operations  reading  updating  deleting      to a huge amount of files distributed among a massive number of computers  then it becomes more interesting  note that a massive number of computers opens the door for more hardware failures because more server machines mean more hardware and thus high probability of hardware failures   two of the most widely used dfs are the google file system and the hadoop distributed file system  in both systems  the file system is implemented by user level processes running on top of a standard operating system  in the case of gfs  linux       gfs and hdfs are specifically built for handling batch processing on very large data sets  for that  the following hypotheses must be taken into account      load balancing is essential for efficient operations in distributed environments  it means distributing the amount of work to do between different servers   in order to get more work done in the same amount of time and serve clients faster  in this case  consider a large scale distributed file system  the system contains n chunkservers in a cloud  n can be              or more   where a certain number of files are stored  each file is split into several parts or chunks of fixed size  for example    megabytes   the load of each chunkserver is proportional to the number of chunks hosted by the server    in a load balanced cloud  the resources can be well used while maximizing the performance of mapreduce based applications     in a cloud computing environment  failure is the norm      and chunkservers may be upgraded  replaced  and added in the system  files can also be dynamically created  deleted  and appended  that leads to load imbalance in a distributed file system  meaning that the file chunks are not distributed equitably between the nodes     distributed file systems in clouds such as gfs and hdfs rely on central servers  master for gfs and namenode for hdfs  to manage the metadata and the load balancing  the master rebalances replicas periodically  data must be moved form a datanode chunkserver to another one if its free space is below a certain threshold    however  this centralized approach can provoke a bottleneck for those servers as they become unable to manage a large number of file accesses  consequently  dealing with the load imbalance problem with the central nodes complicates more the situation as it increases their heavy loads  the load rebalance problem is np hard       in order to manage large number of chunkservers to work in collaboration  and solve the problem of load balancing in distributed file systems  several approaches have been proposed such as reallocating file chunks such that the chunks can be distributed to the system as uniformly as possible while reducing the movement cost as much as possible       among the biggest internet companies  google has created its own distributed file system named google file system to meet the rapidly growing requests of google s data processing needs and it is used for all cloud services  gfs is a scalable distributed file system for data intensive applications  it provides a fault tolerant way to store data and offer a high performance to a large number of clients     gfs uses mapreduce that allows users to create programs and run them on multiple machines without thinking about the parallelization and load balancing issues   gfs architecture is based on a single master  multiple chunkservers and multiple clients       the master server running on a dedicated node is responsible for coordinating storage resources and managing files s metadata  such as the equivalent of inodes in classical file systems    each file is split to multiple chunks of    mbyte  each chunk is stored in a chunk server a chunk is identified by a chunk handle  which is a globally unique    bit number that is assigned by the master when the chunk is first created     as said previously  the master maintain all of the files s metadata including their names  directories and the mapping of files to the list of chunks that contain each file s data the metadata is kept in the master main memory  along with the mapping of files to chunks  updates of these data are logged to the disk onto an operation log  this operation log is also replicated onto remote machines  when the log become too large  a checkpoint is made and the main memory data is stored in a b tree structure to facilitate the mapped back into main memory       for fault tolerance  a chunk is replicated onto multiple chunkservers  by default on three chunckservers    a chunk is available on at least a chunk server  the advantage of this system is the simplicity  the master is responsible of allocating the chunk servers for each chunk and it is contacted only for metadata information  for all other data  the client has to interact with chunkservers     moreover  the master keeps track of where a chunk is located  however  it does not attempt to keep precisely the chunk locations but occasionally contact the chunk servers to see which chunks they have stored    gfs is a scalable distributed file system for data intensive applications    the master does not have a problem of bottleneck due to all the work that it has to accomplish  in fact  when the client want to access data  it communicates with the master to see which chunk server is holding that data  once done  the communication is set up between the client and the concerned chunk server     in gfs  most files are modified by appending new data and not overwriting existing data  in fact  once written  the files are only read and often only sequentially rather than randomly  and that made this dfs the most suitable for scenarios in which many large files are created once but read many times         when a client wants to write update to a file  the master should accord a replica for this operation  this replica will be the primary replica since it is the first one that gets the modification from clients  the process of writing is decomposed into two steps      consequently  we can differentiate two types of flows  the data flow and the control flow  the first one is associated to the sending phase and the second one is associated to the writing phase  this assures that the primary chunk server takes the control of the writes order  note that when the master accord the write operation to a replica  it increments the chunk version number and informs all of the replicas containing that chunk of the new version number  chunk version numbers allow to see if any replica didn t make the update because that chunkserver was down       it seems that some new google applications did not work well with the    megabyte chunk size  to treat that  gfs started in      to implement the bigtable approach       hdfs  hosted by apache software foundation  is a distributed file system designed to hold very large amounts of data  terabytes or even petabytes   its architecture is similar to gfs one  i e  a master slave architecture  the hdfs is normally installed on a cluster of computers  the design concept of hadoop refers to google  including google file system  google mapreduce and bigtable  these three techniques are individually mapping to hadoop and distributed file system  hdfs   hadoop mapreduce hadoop base  hbase        an hdfs cluster consists of a single namenode and several datanode machines  a namenode  a master server  manages and maintains the metadata of storage datanodes in its ram  datanodes manage storage attached to the nodes that they run on  the namenode and datanode are software programs designed to run on everyday use machines  which typically run on a gnu linux os  hdfs can be run on any machine that supports java and therefore can run either a namenode or the datanode software       more explicitly  a file is split into one or more equal size blocks except the last block that could be smaller  each block is stored in multiple datanodes  each block may be replicated on multiple datanodes to guarantee a high availability  by default  each block is replicated three times  a process called  block level replication        the namenode manages the file system namespace operations such as opening  closing  and renaming files and directories and regulates the file access  it also determines the mapping of blocks to datanodes  the datanodes are responsible for operating read and write requests from the file system s clients  managing the block allocation or deletion  and replicating blocks       when a client wants to read or write data  it contacts the namenode and the namenode checks where the data should be read from or written to  after that  the client has the location of the datanode and can send read or write requests to it     the hdfs is typically characterized by its compatibility with data rebalancing schemes  in general  managing the free space on a datanode is very important  data must be moved from one datanode to another one if its free space is not adequate  and in the case of creating additional replicas  data should move to assure the balance of the system       distributed file systems can be classified into two categories  the first category of dfs is the one designed for internet services such as gfs  the second category include dfs that support intensive applications usually executed in parallel    here are some example from the second category  ceph fs  fraunhofer file system  fhgfs   lustre file system  ibm general parallel file system  gpfs  and parallel virtual file system     ceph file system is a distributed file system that provides excellent performance and reliability    it presents some challenges that are the need to be able to deal with huge files and directories  coordinate the activity of thousands of disks  provide parallel access to metadata on a massive scale  manipulate both scientific and general purpose workloads  authenticate and encrypt at scale  and increase or decrease dynamically because of frequent device decommissioning  device failures  and cluster expansions       fhgfs  the high performance parallel file system from the fraunhofer competence centre for high performance computing  the distributed metadata architecture of fhgfs has been designed in order to provide the scalability and flexibility needed to run the most widely used hpc applications       lustre file system has been designed and implemented to deal with the issue of bottlenecks traditionally found in distributed systems  lustre is characterized by its efficiency  scalability and redundancy    gpfs was also designed with the goal of removing the bottlenecks       the high performance of distributed file systems require an efficient communication between computing nodes and a fast access to the storage system  operations as open  close  read  write  send and receive should be fast to assure that performance  note that for each read or write request  the remote disk is accessed and that may takes a long time due to the network latencies       the data communication  send receive  operation transfer the data from the application buffer to the kernel on the machine tcp control the process of sending data and is implemented in the kernel  however  in case of network congestion or errors  tcp may not send the data directly  while transferring  data from a buffer in the kernel to the application  the machine does not read the byte stream from the remote machine  in fact  tcp is responsible for buffering the data for the application       providing a high level of communication can be done by choosing the buffer size of file reading and writing or file sending and receiving on application level  explicitly  the buffer mechanism is developed using circular linked list    it consists of a set of buffernodes  each buffernode has a datafield  the datafield contains the data and a pointer called nextbuffernode that points to the next buffernode  to find out the current position  two pointers are used  currentbuffernode and endbuffernode  that represent the position in the buffernode for the last written position and last read one  if the buffernode has no free space  it will send a wait signal to the client to tell him to wait until there is available space       more and more users have multiple devices with ad hoc connectivity  these devices need to be synchronized  in fact  an important point is to maintain user data by synchronizing replicated data sets between an arbitrary number of servers  this is useful for the backups and also for offline operation  indeed  when the user network conditions are not good  then the user device will selectively replicate a part of data that will be modified later and off line  once the network conditions become good  it makes the synchronization    two approaches exists to tackle with the distributed synchronization issue  the user controlled peer to peer synchronization and the cloud master replica synchronization approach       in cloud computing  the most important security concepts are confidentiality  availability and integrity  in fact  confidentiality becomes indispensable in order to keep private data from being disclosed and maintain privacy  in addition  integrity assures that data is not corrupted       confidentiality means that data and computation tasks are confidential  neither the cloud provider nor others clients could access to data  much research has been done about confidentiality because it is one of the crucial points that still represents challenges for cloud computing  the lack of trust toward the cloud providers is also a related issue    so the infrastructure of the cloud must make assurance that all consumer s data will not be accessed by any an unauthorized persons  the environment becomes unsecured if the service provider       if these three conditions are satisfied simultaneously  then it became very dangerous     the geographic location of data stores influences on the privacy and confidentiality  furthermore  the location of clients should be taken into account  indeed  clients in europe won t be interested by using datacenters located in united states  because that affects the confidentiality of data as it will not be guaranteed  in order to figure out that problem  some cloud computing vendors have included the geographic location of the hosting as a parameter of the service level agreement made with the customer    allowing users to choose by themselves the locations of the servers that will host their data     an approach that may help to face the confidentiality matter is the data encryption    otherwise  there will be some serious risks of unauthorized uses  in the same context  other solutions exists such as encrypting only sensitive data    and supporting only some operations  in order to simplify computation    furthermore  cryptographic techniques and tools as fhe  are also used to strengthen privacy preserving in cloud       availability is generally treated by replication           meanwhile  consistency must be guaranteed  however  consistency and availability cannot be achieved at the same time  this means that neither releasing consistency will allow the system to remain available nor making consistency a priority and letting the system sometimes unavailable    in other hand  data must have an identity to be accessible  for instance  skute    is a mechanism based on key value store that allow dynamic data allocation in an efficient way  indeed  each server must be identified by a label in this form  continent country datacenter room rack server   the server has reference to multiple virtual nodes  each node has a selection of data or multiple partition of multiple data   each data is identified by a key space which is generated by a one way cryptographic hash function  e g  md   and is localised by the hash function value of this key  the key space may be partitioned into multiple partitions and every partition refers to a part of a data  to perform replication  virtual nodes must be replicated and so referenced by other servers  to maximize data availability data durability  the replicas must be placed in different servers and every server should be in different region  because data availability increase with the geographical diversity  the process of replication consists of an evaluation of the data availability that must be above a certain minimum  otherwise  data are replicated to another chunk server  each partition i has an availability value represented by the following formula         where  are the servers hosting the replicas   and  are the confidence of servers  and   relying on technical factors such as hardware components and non technical ones like the economic and political situation of a country  and the diversity is the geographical distance between and        replication is a great solution to ensure data availability  but it costs too much in terms of memory space    diskreduce    is a modified version of hdfs that s based on raid technology  raid   and raid    and allows asynchronous encoding of replicated data  indeed  there is a background process which look for wide data and it deletes extra copies after encoding it  another approach is to replace replication with erasure coding   in addition  to ensure data availability there are many approaches that allow data recovery  in fact  data must be coded and once it is lost  it can be recovered from fragments which are constructed during the coding phase    some other approaches that apply different mechanisms to guarantee availability are following  reed solomon code of microsoft azure  raidnode for hdfs  also google is still working on a new approach based on erasure coding mechanism       until now there is no raid implementation established for cloud storage       integrity in cloud computing implies data integrity and meanwhile computing integrity  integrity means data has to be stored correctly on cloud servers and in case of failures or incorrect computing  problems have to be detected     data integrity is easy to achieve thanks to cryptography  typically through message authentication code  or macs  on data blocks        there are different ways affecting data s integrity either from a malicious event or from administration errors  i e  backup and restore  data migration  changing memberships in p p systems        it exists some checking mechanisms that check data integrity  for instance     the cloud computing is growing rapidly  the us government decided to spend     of annual growth rate cagr and fixed   billion dollars by       huge number that should be take into consideration       more and more companies have been utilizing the cloud computing to manage the massive amount of data and overcome the lack of storage capacities  indeed  the companies are enabled to use resources as a service to assure their computing needs without having to invest on infrastructure  so they pay for what they are going to use  pay as you go model        every application provider has to periodically pay the cost of each server where replicas of his data are stored  the cost of a server is generally estimated by the quality of the hardware  the storage capacities  and its query processing and communication overhead       cloud computing facilitates the tasks for enterprises to scale their services under the client requests  the pay as you go model has also facilitate the tasks for the startup companies that wish to benefit from compute intensive business  cloud computing also offers a huge opportunity to many third world countries that don t have enough resources  and thus enabling it services  cloud computing can lower it barriers to innovation       although the wide utilization of cloud computing  an efficient sharing of large volumes of data in an untrusted cloud is still a challenging research topic  