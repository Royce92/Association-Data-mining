 a computer cluster consists of a set of loosely or tightly connected computers that work together so that  in many respects  they can be viewed as a single system  unlike grid computers  computer clusters have each node set to perform the same task  controlled and scheduled by software      the components of a cluster are usually connected to each other through fast local area networks   lan    with each node  computer used as a server  running its own instance of an operating system  in most circumstances  all of the nodes use the same hardware  and the same operating system  although in some setups  i e  using open source cluster application resources  oscar    different operating systems can be used on each computer  and or different hardware      they are usually deployed to improve performance and availability over that of a single computer  while typically being much more cost effective than single computers of comparable speed or availability      computer clusters emerged as a result of convergence of a number of computing trends including the availability of low cost microprocessors  high speed networks  and software for high performance distributed computing citation needed they have a wide range of applicability and deployment  ranging from small business clusters with a handful of nodes to some of the fastest supercomputers in the world such as ibm s sequoia   the applications that can be done however  are nonetheless limited  since the software needs to be purpose built per task  it is hence not possible to use computer clusters for casual computing tasks  clarification needed            the desire to get more computing power and better reliability by orchestrating a number of low cost commercial off the shelf computers has given rise to a variety of architectures and configurations     the computer clustering approach usually  but not always  connects a number of readily available computing nodes  e g  personal computers used as servers  via a fast local area network   the activities of the computing nodes are orchestrated by  clustering middleware   a software layer that sits atop the nodes and allows the users to treat the cluster as by and large one cohesive computing unit  e g  via a single system image concept      computer clustering relies on a centralized management approach which makes the nodes available as orchestrated shared servers  it is distinct from other approaches such as peer to peer or grid computing which also use many nodes  but with a far more distributed nature      a computer cluster may be a simple two node system which just connects two personal computers  or may be a very fast supercomputer  a basic approach to building a cluster is that of a beowulf cluster which may be built with a few personal computers to produce a cost effective alternative to traditional high performance computing  an early project that showed the viability of the concept was the     nodes stone soupercomputer   the developers used linux  the parallel virtual machine toolkit and the message passing interface library to achieve high performance at a relatively low cost      although a cluster may consist of just a few personal computers connected by a simple network  the cluster architecture may also be used to achieve very high levels of performance  the top    organization s semiannual list of the     fastest supercomputers often includes many clusters  e g  the world s fastest machine in      was the k computer which has a distributed memory  cluster architecture         greg pfister has stated that clusters were not invented by any specific vendor but by customers who could not fit all their work on one computer  or needed a backup    pfister estimates the date as some time in the     s  the formal engineering basis of cluster computing as a means of doing parallel work of any sort was arguably invented by gene amdahl of ibm  who in      published what has come to be regarded as the seminal paper on parallel processing  amdahl s law     the history of early computer clusters is more or less directly tied into the history of early networks  as one of the primary motivations for the development of a network was to link computing resources  creating a de facto computer cluster     the first production system designed as a cluster was the burroughs b     in the mid     s  this allowed up to four computers  each with either one or two processors  to be tightly coupled to a common disk storage subsystem in order to distribute the workload  unlike standard multiprocessor systems  each computer could be restarted without disrupting overall operation     the first commercial loosely coupled clustering product was datapoint corporation s  attached resource computer   arc  system  developed in       and using arcnet as the cluster interface  clustering per se did not really take off until digital equipment corporation released their vaxcluster product in      for the vax vms operating system  now named as openvms   the arc and vaxcluster products not only supported parallel computing  but also shared file systems and peripheral devices  the idea was to provide the advantages of parallel processing  while maintaining data reliability and uniqueness  two other noteworthy early commercial clusters were the tandem himalayan  a circa      high availability product  and the ibm s     parallel sysplex  also circa       primarily for business use      within the same time frame  while computer clusters used parallelism outside the computer on a commodity network  supercomputers began to use them within the same computer  following the success of the cdc      in       the cray   was delivered in       and introduced internal parallelism via vector processing    while early supercomputers excluded clusters and relied on shared memory  in time some of the fastest supercomputers  e g  the k computer  relied on cluster architectures     computer clusters may be configured for different purposes ranging from general purpose business needs such as web service support  to computation intensive scientific calculations  in either case  the cluster may use a high availability approach  note that the attributes described below are not exclusive and a  computer cluster  may also use a high availability approach  etc      load balancing  clusters are configurations in which cluster nodes share computational workload to provide better overall performance  for example  a web server cluster may assign different queries to different nodes  so the overall response time will be optimized    however  approaches to load balancing may significantly differ among applications  e g  a high performance cluster used for scientific computations would balance load with different algorithms from a web server cluster which may just use a simple round robin method by assigning each new request to a different node       computer clusters are used for computation intensive purposes  rather than handling io oriented operations such as web service or databases    for instance  a computer cluster might support computational simulations of vehicle crashes or weather  very tightly coupled computer clusters are designed for work that may approach  supercomputing       high availability clusters   also known as failover clusters  or ha clusters  improve the availability of the cluster approach  they operate by having redundant nodes  which are then used to provide service when system components fail  ha cluster implementations attempt to use redundancy of cluster components to eliminate single points of failure  there are commercial implementations of high availability clusters for many operating systems  the linux ha project is one commonly used free software ha package for the linux operating system     clusters are primarily designed with performance in mind  but installations are based on many other factors  fault tolerance  the ability for a system to continue working with a malfunctioning node  also allows for simpler scalability  and in high performance situations  low frequency of maintenance routines  resource consolidationclarification needed  and centralized management         one of the issues in designing a cluster is how tightly coupled the individual nodes may be  for instance  a single computer job may require frequent communication among nodes  this implies that the cluster shares a dedicated network  is densely located  and probably has homogeneous nodes  the other extreme is where a computer job uses one or few nodes  and needs little or no inter node communication  approaching grid computing     in a beowulf system  the application programs never see the computational nodes  also called slave computers  but only interact with the  master  which is a specific computer handling the scheduling and management of the slaves    in a typical implementation the master has two network interfaces  one that communicates with the private beowulf network for the slaves  the other for the general purpose network of the organization    the slave computers typically have their own version of the same operating system  and local memory and disk space  however  the private slave network may also have a large and shared file server that stores global persistent data  accessed by the slaves as needed       by contrast  the special purpose     node degima cluster is tuned to running astrophysical n body simulations using the multiple walk parallel treecode  rather than general purpose scientific computations       due to the increasing computing power of each generation of game consoles  a novel use has emerged where they are repurposed into high performance computing  hpc  clusters  some examples of game console clusters are sony playstation clusters and microsoft xbox clusters  another example of consumer game product is the nvidia tesla personal supercomputer workstation  which uses multiple graphics accelerator processor chips  besides game consoles  high end graphics cards too can be used instead  the use of graphics cards  or rather their gpu s  to do calculations for grid computing is vastly more economical than using cpu s  despite being less precise  however  when using double precision values  they become as precise to work with as cpu s  and still be much less costly  purchase cost        computer clusters have historically run on separate physical computers with the same operating system  with the advent of virtualization  the cluster nodes may run on separate physical computers with different operating systems which are painted above with a virtual layer to look similar   citation neededclarification needed the cluster may also be virtualized on various configurations as maintenance takes place  an example implementation is xen as the virtualization manager with linux ha       as the computer clusters were appearing during the     s  so were supercomputers  one of the elements that distinguished the three classes at that time was that the early supercomputers relied on shared memory  to date clusters do not typically use physically shared memory  while many supercomputer architectures have also abandoned it     however  the use of a clustered file system is essential in modern computer clusters citation needed examples include the ibm general parallel file system  microsoft s cluster shared volumes or the oracle cluster file system     two widely used approaches for communication between cluster nodes are mpi  the message passing interface and pvm  the parallel virtual machine       pvm was developed at the oak ridge national laboratory around      before mpi was available  pvm must be directly installed on every cluster node and provides a set of software libraries that paint the node as a  parallel virtual machine   pvm provides a run time environment for message passing  task and resource management  and fault notification  pvm can be used by user programs written in c  c    or fortran  etc         mpi emerged in the early     s out of discussions among    organizations  the initial effort was supported by arpa and national science foundation  rather than starting anew  the design of mpi drew on various features available in commercial systems of the time  the mpi specifications then gave rise to specific implementations  mpi implementations typically use tcp ip and socket connections    mpi is now a widely available communications model that enables parallel programs to be written in languages such as c  fortran  python  etc    thus  unlike pvm which provides a concrete implementation  mpi is a specification which has been implemented in systems such as mpich and open mpi         one of the challenges in the use of a computer cluster is the cost of administrating it which can at times be as high as the cost of administrating n independent machines  if the cluster has n nodes    in some cases this provides an advantage to shared memory architectures with lower administration costs    this has also made virtual machines popular  due to the ease of administration       when a large multi user cluster needs to access very large amounts of data  task scheduling becomes a challenge  in a heterogeneous cpu gpu cluster with a complex application environment  the performance of each job depends on the characteristics of the underlying cluster  therefore  mapping tasks onto cpu cores and gpu devices provides significant challenges    this is an area of ongoing research  algorithms that combine and extend mapreduce and hadoop have been proposed and studied       when a node in a cluster fails  strategies such as  fencing  may be employed to keep the rest of the system operational      fencing is the process of isolating a node or protecting shared resources when a node appears to be malfunctioning  there are two classes of fencing methods  one disables a node itself  and the other disallows access to resources such as shared disks       the stonith method stands for  shoot the other node in the head   meaning that the suspected node is disabled or powered off  for instance  power fencing uses a power controller to turn off an inoperable node       the resources fencing approach disallows access to resources without powering off the node  this may include persistent reservation fencing via the scsi   fibre channel fencing to disable the fibre channel port  or global network block device  gnbd  fencing to disable access to the gnbd server     load balancing clusters such as web servers use cluster architectures to support a large number of users and typically each user request is routed to a specific node  achieving task parallelism without multi node cooperation  given that the main goal of the system is providing rapid user access to shared data  however   computer clusters  which perform complex computations for a small number of users need to take advantage of the parallel processing capabilities of the cluster and partition  the same computation  among several nodes       automatic parallelization of programs continues to remain a technical challenge  but parallel programming models can be used to effectuate a higher degree of parallelism via the simultaneous execution of separate portions of a program on different processors         the development and debugging of parallel programs on a cluster requires parallel language primitives as well as suitable tools such as those discussed by the high performance debugging forum  hpdf  which resulted in the hpd specifications      tools such as totalview were then developed to debug parallel implementations on computer clusters which use mpi or pvm for message passing     the berkeley now  network of workstations  system gathers cluster data and stores them in a database  while a system such as parmon  developed in india  allows for the visual observation and management of large clusters       application checkpointing can be used to restore a given state of the system when a node fails during a long multi node computation    this is essential in large clusters  given that as the number of nodes increases  so does the likelihood of node failure under heavy computational loads  checkpointing can restore the system to a stable state so that processing can resume without having to recompute results       the gnu linux world supports various cluster software  for application clustering  there is distcc  and mpich  linux virtual server  linux ha   director based clusters that allow incoming requests for services to be distributed across multiple cluster nodes  mosix  linuxpmi  kerrighed  openssi are full blown clusters integrated into the kernel that provide for automatic process migration among homogeneous nodes  openssi  openmosix and kerrighed are single system image implementations     microsoft windows computer cluster server      based on the windows server platform provides pieces for high performance computing like the job scheduler  msmpi library and management tools     glite is a set of middleware technologies created by the enabling grids for e science  egee  project     slurm is also used to schedule and manage some of the largest supercomputer clusters  see top    list      although most computer clusters are permanent fixtures  attempts at flash mob computing have been made to build short lived clusters for specific computations  however  larger scale volunteer computing systems such as boinc based systems have had more followers     basic concepts    distributed computing    specific systems    computer farms 