 the world wide web  www  w   is an information system of interlinked hypertext documents that are accessed via the internet   it has also commonly become known simply as the web  individual document pages on the world wide web are called web pages and are accessed with a software application running on the user s computer  commonly called a web browser  web pages may contain text  images  videos  and other multimedia components  as well as web navigation features consisting of hyperlinks     tim berners lee  a british computer scientist and former cern employee   is the inventor of the web  on    march        berners lee wrote a proposal for what would eventually become the world wide web   the      proposal was meant for a more effective cern communication system but berners lee eventually realised the concept could be implemented throughout the world   berners lee and belgian computer scientist robert cailliau proposed in      to use hypertext  to link and access information of various kinds as a web of nodes in which the user can browse at will    and berners lee finished the first website in december of that year   the first test was completed around    december      and berners lee reported about the project on the newsgroup alt hypertext on   august                   on march           tim berners lee wrote a proposal to the management at cern that referenced enquire  a database and software project he had built in       and described a more elaborate information management system based on links embedded in readable text   imagine  then  the references in this document all being associated with the network address of the thing to which they referred  so that while reading this document you could skip to them with a click of the mouse   such a system  he explained  could be referred to using one of the existing meanings of the word hypertext  a term that he says was coined in the     s  there is no reason  the proposal continues  why such hypertext links could not encompass multimedia documents including graphics  speech and video  so that berners lee goes on to propose the term hypermedia      with help from robert cailliau  he published a more formal proposal  on    november       to build a  hypertext project  called  worldwideweb   one word  also  w    as a  web  of  hypertext documents  to be viewed by  browsers  using a client server architecture   this proposal estimated that a read only web would be developed within three months and that it would take six months to achieve  the creation of new links and new material by readers  so that authorship becomes universal  as well as  the automatic notification of a reader when new material of interest to him her has become available   while the read only goal was met  accessible authorship of web content took longer to mature  with the wiki concept  webdav  blogs  web     and rss atom       the proposal was modeled after the sgml reader dynatext by electronic book technology  a spin off from the institute for research in information and scholarship at brown university  the dynatext system  licensed by cern  was a key player in the extension of sgml iso           to hypermedia within hytime  but it was considered too expensive and had an inappropriate licensing policy for use in the general high energy physics community  namely a fee for each document and each document alteration     a next computer was used by berners lee as the world s first web server and also to write the first web browser  worldwideweb  in       by christmas       berners lee had built all the tools necessary for a working web    the first web browser  which was a web editor as well   the first web server  and the first web pages    which described the project itself     the first web page may be lost  but paul jones of unc chapel hill in north carolina announced in may      that berners lee gave him what he says is the oldest known web page during a      visit to unc  jones stored it on a magneto optical drive and on his next computer       on   august       berners lee published a short summary of the world wide web project on the newsgroup alt hypertext    this date also marked the debut of the web as a publicly available service on the internet  although new users only access it after august     for this reason this is considered the internaut s day  several newsmedia have reported that the first photo on the web was published by berners lee in       an image of the cern house band les horribles cernettes taken by silvano de gennaro  gennaro has disclaimed this story  writing that media were  totally distorting our words for the sake of cheap sensationalism        the first server outside europe was installed at the stanford linear accelerator center  slac  in palo alto  california  to host the spires hep database  accounts differ substantially as to the date of this event  the world wide web consortium says december         whereas slac itself claims           this is supported by a w c document titled a little history of the world wide web       the underlying concept of hypertext originated in previous projects from the     s  such as the hypertext editing system  hes  at brown university  ted nelson s project xanadu  and douglas engelbart s on line system  nls   both nelson and engelbart were in turn inspired by vannevar bush s microfilm based memex  which was described in the      essay  as we may think        berners lee s breakthrough was to marry hypertext to the internet  in his book weaving the web  he explains that he had repeatedly suggested that a marriage between the two technologies was possible to members of both technical communities  but when no one took up his invitation  he finally assumed the project himself  in the process  he developed three essential technologies     the world wide web had a number of differences from other hypertext systems available at the time  the web required only unidirectional links rather than bidirectional ones  making it possible for someone to link to another resource without action by the owner of that resource  it also significantly reduced the difficulty of implementing web servers and browsers  in comparison to earlier systems   but in turn presented the chronic problem of link rot  unlike predecessors such as hypercard  the world wide web was non proprietary  making it possible to develop servers and clients independently and to add extensions without licensing restrictions  on    april       cern announced that the world wide web would be free to anyone  with no fees due    coming two months after the announcement that the server implementation of the gopher protocol was no longer free to use  this produced a rapid shift away from gopher and towards the web  an early popular web browser was violawww for unix and the x windowing system     scholars generally agree that a turning point for the world wide web began with the introduction   of the mosaic web browser   in       a graphical browser developed by a team at the national center for supercomputing applications at the university of illinois at urbana champaign  ncsa uiuc   led by marc andreessen  funding for mosaic came from the u s  high performance computing and communications initiative and the high performance computing and communication act of       one of several computing developments initiated by u s  senator al gore    prior to the release of mosaic  graphics were not commonly mixed with text in web pages and the web s popularity was less than older protocols in use over the internet  such as gopher and wide area information servers  wais   mosaic s graphical user interface allowed the web to become  by far  the most popular internet protocol     the world wide web consortium  w c  was founded by tim berners lee after he left the european organization for nuclear research  cern  in october       it was founded at the massachusetts institute of technology laboratory for computer science  mit lcs  with support from the defense advanced research projects agency  darpa   which had pioneered the internet  a year later  a second site was founded at inria  a french national computer research lab  with support from the european commission dg infso  and in       a third continental site was created in japan at keio university  by the end of       the total number of websites was still relatively small  but many notable websites were already active that foreshadowed or inspired today s most popular services     connected by the existing internet  other websites were created around the world  adding international standards for domain names and html  since then  berners lee has played an active role in guiding the development of web standards  such as the markup languages to compose web pages in   and has advocated his vision of a semantic web  the world wide web enabled the spread of information over the internet through an easy to use and flexible format  it thus played an important role in popularizing use of the internet    although the two terms are sometimes conflated in popular use  world wide web is not synonymous with internet    the web is an information space containing hyperlinked documents and other resources  identified by their uris    it is implemented as both client and server software using internet protocols such as tcp ip and http     tim berners lee was knighted in      by queen elizabeth ii for  services to the global development of the internet          the terms internet and world wide web are often used without much distinction  however  the two things are not the same  the internet is a global system of interconnected computer networks  in contrast  the world wide web is one of the services transferred over these networks  it is a collection of text documents and other resources  linked by hyperlinks and urls  usually accessed by web browsers  from web servers       viewing a web page on the world wide web normally begins either by typing the url of the page into a web browser  or by following a hyperlink to that page or resource  the web browser then initiates a series of background communication messages to fetch and display the requested page  in the     s  using a browser to view web pages and to move from one web page to another through hyperlinks came to be known as  browsing    web surfing    after channel surfing   or  navigating the web   early studies of this new behavior investigated user patterns in using web browsers  one study  for example  found five user patterns  exploratory surfing  window surfing  evolved surfing  bounded navigation and targeted navigation       the following example demonstrates the functioning of web browser when accessing a page at the url http   example org wiki world_wide_web  the browser resolves the server name of the url  example org  into an internet protocol address using the globally distributed domain name system  dns   this lookup returns an ip address such as              the browser then requests the resource by sending an http request across the internet to the computer at that address  it requests service from a specific tcp port number that is well known for the http service  so that the receiving host can distinguish an http request from other network protocols it may be servicing  the http protocol normally uses port number     the content of the http request can be as simple as two lines of text     the computer receiving the http request delivers it to web server software listening for requests on port     if the web server can fulfill the request it sends an http response back to the browser indicating success     followed by the content of the requested page  the hypertext markup language for a basic web page looks like  html   head   title example org   the world wide web  title    head   body   p the world wide web  abbreviated as www and commonly known      p    body    html     the web browser parses the html and interprets the markup   title    p  for paragraph  and such  that surrounds the words to format the text on the screen  many web pages use html to reference the urls of other resources such as images  other embedded media  scripts that affect page behavior  and cascading style sheets that affect page layout  the browser makes additional http requests to the web server for these other internet media types  as it receives their content from the web server  the browser progressively renders the page onto the screen as specified by its html and these additional resources     most web pages contain hyperlinks to other related pages and perhaps to downloadable files  source documents  definitions and other web resources  in the underlying html  a hyperlink looks like  a href  http   example org wiki main_page  example org  a free encyclopedia  a     such a collection of useful  related resources  interconnected via hypertext links is dubbed a web of information  publication on the internet created what tim berners lee first called the worldwideweb  in its original camelcase  which was subsequently discarded  in november           the hyperlink structure of the www is described by the webgraph  the nodes of the webgraph correspond to the web pages  or urls  the directed edges between them to the hyperlinks     over time  many web resources pointed to by hyperlinks disappear  relocate  or are replaced with different content  this makes hyperlinks obsolete  a phenomenon referred to in some circles as link rot and the hyperlinks affected by it are often called dead links  the ephemeral nature of the web has prompted many efforts to archive web sites  the internet archive  active since       is the best known of such efforts     javascript is a scripting language that was initially developed in      by brendan eich  then of netscape  for use within web pages    the standardised version is ecmascript    to make web pages more interactive  some web applications also use javascript techniques such as ajax  asynchronous javascript and xml   client side script is delivered with the page that can make additional http requests to the server  either in response to user actions such as mouse movements or clicks  or based on elapsed time  the server s responses are used to modify the current page rather than creating a new page with each response  so the server needs only to provide limited  incremental information  multiple ajax requests can be handled at the same time  and users can interact with the page while data is retrieved  web pages may also regularly poll the server to check whether new information is available       many hostnames used for the world wide web begin with www because of the long standing practice of naming internet hosts according to the services they provide  the hostname of a web server is often www  in the same way that it may be ftp for an ftp server  and news or nntp for a usenet news server  these host names appear as domain name system  dns  or subdomain names  as in www example com  the use of www is not required by any technical or policy standard and many web sites do not use it  indeed  the first ever web server was called nxoc   cern ch    according to paolo palazzi    who worked at cern along with tim berners lee  the popular use of www as subdomain was accidental  the world wide web project page was intended to be published at www cern ch while info cern ch was intended to be the cern home page  however the dns records were never switched  and the practice of prepending www to an institution s website domain name was subsequently copied  many established websites still use the prefix  or they employ other subdomain names such as www   secure or en for special purposes  many such web servers are set up so that both the main domain name  e g   example com  and the www subdomain  e g   www example com  refer to the same site  others require one form or the other  or they may map to different web sites     the use of a subdomain name is useful for load balancing incoming web traffic by creating a cname record that points to a cluster of web servers  since  currently  only a subdomain can be used in a cname  the same result cannot be achieved by using the bare domain root citation needed    when a user submits an incomplete domain name to a web browser in its address bar input field  some web browsers automatically try adding the prefix  www  to the beginning of it and possibly   com     org  and   net  at the end  depending on what might be missing  for example  entering  microsoft  may be transformed to http   www microsoft com  and  openoffice  to http   www openoffice org  this feature started appearing in early versions of mozilla firefox  when it still had the working title  firebird  in early       from an earlier practice in browsers such as lynx    it is reported that microsoft was granted a us patent for the same idea in       but only for mobile devices       in english  www is usually read as double u double u double u    some users pronounce it dub dub dub  particularly in new zealand  stephen fry  in his  podgrammes  series of podcasts  pronounces it wuh wuh wuh citation needed the english writer douglas adams once quipped in the independent on sunday          the world wide web is the only thing i know of whose shortened form takes three times longer to say than what it s short for  citation needed in mandarin chinese  world wide web is commonly translated via a phono semantic matching to w n w i w ng        which satisfies www and literally means  myriad dimensional net     a translation that reflects the design concept and proliferation of the world wide web  tim berners lee s web space states that world wide web is officially spelled as three separate words  each capitalised  with no intervening hyphens       use of the www prefix is declining as web     web applications seek to brand their domain names and make them easily pronounceable    as the mobile web grows in popularity  services like gmail com  myspace com  facebook com and twitter com are most often mentioned without adding  www    or  indeed    com   to the domain     the scheme specifiers http    and https    at the start of a web uri refer to hypertext transfer protocol or http secure  respectively  they specify the communication protocol to use for the request and response  the http protocol is fundamental to the operation of the world wide web  and the added encryption layer in https is essential when browsers send or retrieve confidential data  such as passwords or banking information  web browsers usually automatically prepend http    to user entered uris  if omitted     the primary function of a web server is to deliver web pages in response to client requests  this means delivery of html documents and any additional content that may be included by a document  such as images  style sheets and scripts     for criminals  the web has become the preferred way to spread malware  cybercrime on the web can include identity theft  fraud  espionage and intelligence gathering    web based vulnerabilities now outnumber traditional computer security concerns      and as measured by google  about one in ten web pages may contain malicious code    most web based attacks take place on legitimate websites  and most  as measured by sophos  are hosted in the united states  china and russia    the most common of all malware threats is sql injection attacks against websites    through html and uris  the web was vulnerable to attacks like cross site scripting  xss  that came with the introduction of javascript   and were exacerbated to some degree by web     and ajax web design that favors the use of scripts    today by one estimate      of all websites are open to xss attacks on their users    phishing is another common threat to the web   sa  the security division of emc  today announced the findings of its january      fraud report  estimating the global losses from phishing at      billion in           two of the well known phishing methods are covert redirect and open redirect     proposed solutions vary to extremes  large security vendors like mcafee already design governance and compliance suites to meet post      regulations    and some  like finjan have recommended active real time inspection of code and all content regardless of its source    some have argued that for enterprise to see security as a business opportunity rather than a cost center     ubiquitous  always on digital rights management  enforced in the infrastructure by a handful of organizations must replace the hundreds of companies that today secure data and networks    jonathan zittrain has said users sharing responsibility for computing safety is far preferable to locking down the internet       every time a client requests a web page  the server can identify the request s ip address and usually logs it  also  unless set not to do so  most web browsers record requested web pages in a viewable history feature  and usually cache much of the content locally  unless the server browser communication uses https encryption  web requests and responses travel in plain text across the internet and can be viewed  recorded  and cached by intermediate systems     when a web page asks for  and the user supplies  personally identifiable information such as their real name  address  e mail address  etc  web based entities can associate current web traffic with that individual  if the website uses http cookies  username and password authentication  or other tracking techniques  it can relate other web visits  before and after  to the identifiable information provided  in this way it is possible for a web based organisation to develop and build a profile of the individual people who use its site or sites  it may be able to build a record for an individual that includes information about their leisure activities  their shopping interests  their profession  and other aspects of their demographic profile  these profiles are obviously of potential interest to marketeers  advertisers and others  depending on the website s terms and conditions and the local laws that apply information from these profiles may be sold  shared  or passed to other organisations without the user being informed  for many ordinary people  this means little more than some unexpected e mails in their in box  or some uncannily relevant advertising on a future web page  for others  it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome  law enforcement  counter terrorism and espionage agencies can also identify  target and track individuals based on their interests or proclivities on the web     social networking sites try to get users to use their real names  interests  and locations  they believe this makes the social networking experience more realistic  and therefore more engaging for all their users  on the other hand  uploaded photographs or unguarded statements can be identified to an individual  who may regret this exposure  employers  schools  parents  and other relatives may be influenced by aspects of social networking profiles that the posting individual did not intend for these audiences  on line bullies may make use of personal information to harass or stalk users  modern social networking websites allow fine grained control of the privacy settings for each individual posting  but these can be complex and not easy to find or use  especially for beginners       photographs and videos posted onto websites have caused particular problems  as they can add a person s face to an on line profile  with modern and potential facial recognition technology  it may then be possible to relate that face with other  previously anonymous  images  events and scenarios that have been imaged elsewhere  because of image caching  mirroring and copying  it is difficult to remove an image from the world wide web     the intellectual property rights for any creative work initially rest with its creator  web users who want to publish their work onto the world wide web  however  must be aware of the details of the way they do it  if artwork  photographs  writings  poems  or technical innovations are published by their creator onto a privately owned web server  then they may choose the copyright and other conditions freely themselves  this is unusual though  more commonly work is uploaded to websites and servers that are owned by other organizations  it depends upon the terms and conditions of the site or service provider to what extent the original owner automatically signs over rights to their work by the choice of destination and by the act of uploading citation needed    some web users erroneously assume that anything they find online is freely available  as if it were in the public domain  which is not always the case  content owners aware of this belief may expect that others will use their published content without permission  therefore  some content publishers embed digital watermarks in media files  sometimes charging users to receive unmarked copies for legitimate use  digital rights management includes forms of access control technology that further limit the use of digital content even after it has been bought or downloaded citation needed    many formal standards and other technical specifications and software define the operation of different aspects of the world wide web  the internet  and computer information exchange  many of the documents are the work of the world wide web consortium  w c   headed by berners lee  but some are produced by the internet engineering task force  ietf  and other organizations     usually  when web standards are discussed  the following publications are seen as foundational     additional publications provide definitions of other essential technologies for the world wide web  including  but not limited to  the following     there are methods for accessing the web in alternative mediums and formats to facilitate use by individuals with disabilities  these disabilities may be visual  auditory  physical  speech related  cognitive  neurological  or some combination  accessibility features also help people with temporary disabilities  like a broken arm  or aging users as their abilities change    the web receives information as well as providing information and interacting with society  the world wide web consortium claims it essential that the web be accessible  so it can provide equal access and equal opportunity to people with disabilities    tim berners lee once noted   the power of the web is in its universality  access by everyone regardless of disability is an essential aspect     many countries regulate web accessibility as a requirement for websites    international cooperation in the w c web accessibility initiative led to simple guidelines that web content authors as well as software developers can use to make the web accessible to persons who may or may not be using assistive technology         the w c internationalization activity assures that web technology works in all languages  scripts  and cultures    beginning in      or       unicode gained ground and eventually in december      surpassed both ascii and western european as the web s most frequently used character encoding    originally rfc      allowed resources to be identified by uri in a subset of us ascii  rfc      allows more characters any character in the universal character set and now a resource can be identified by iri in any language       between      and       the number of web users doubled  and was expected to surpass two billion in         early studies in      and      estimating the size of the web using capture recapture methods showed that much of the web was not indexed by search engines and the web was much larger than expected      according to a      study  there was a massive number  over     billion  of documents on the web  mostly in the invisible web  or deep web    a      survey of       million web pages   determined that by far the most web content was in the english language         next were pages in german         french         and japanese         a more recent study  which used web searches in    different languages to sample the web  determined that there were over      billion web pages in the publicly indexable web as of the end of january         as of march     update  the indexable web contains at least       billion pages    on    july       google software engineers jesse alpert and nissan hajaj announced that google search had discovered one trillion unique urls    as of may     update  over       million domains operated   not in citation given of these     were commercial or other domains operating in the  com generic top level domain       statistics measuring a website s popularity are usually based either on the number of page views or on associated server  hits   file requests  that it receives     frustration over congestion issues in the internet infrastructure and the high latency that results in slow browsing has led to a pejorative name for the world wide web  the world wide wait    speeding up the internet is an ongoing discussion over the use of peering and qos technologies  other solutions to reduce the congestion can be found at w c    guidelines for web response times are       if a user revisits a web page after a short interval  the browser may not need to re obtain the page data from the source web server  almost all web browsers cache recently obtained data  usually on the local hard drive  http requests from a browser usually ask only for data that has changed since the last download  if locally cached data is still current  the browser reuses it  caching reduces the amount of web traffic on the internet  decisions about expiration are made independently for each downloaded file  whether image  stylesheet  javascript  html  or other web resource  thus even on sites with highly dynamic content  many basic resources refresh only occasionally  web site designers find it worthwhile to collate resources such as css data and javascript into a few site wide files so that they can be cached efficiently  this helps reduce page download times and lowers demands on the web server     there are other components of the internet that can cache web content  corporate and academic firewalls often cache web resources requested by one user for the benefit of all   see also caching proxy server   some search engines also store cached content from websites  apart from the facilities built into web servers that can determine when files have been updated and so must be re sent  designers of dynamically generated web pages can control the http headers sent back to requesting users  so that transient or sensitive pages are not cached  internet banking and news sites frequently use this facility  data requested with an http  get  is likely to be cached if other conditions are met  data obtained in response to a  post  is assumed to depend on the data that was posted and so is not cached  