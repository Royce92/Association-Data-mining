 a supercomputer is a computer that has world classclarification needed computational capacity  in       such machines can perform quadrillions of floating point operations per second      supercomputers were introduced in the     s  made initially  and for decades primarily  by seymour cray at control data corporation  cdc   cray research and subsequent companies bearing his name or monogram  while the supercomputers of the     s used only a few processors  in the     s machines with thousands of processors began to appear and  by the end of the   th century  massively parallel supercomputers with tens of thousands of  off the shelf  processors were the norm    as of november     update  china s tianhe   supercomputer is the fastest in the world at      xa petaflops  pflops   or       quadrillion floating point operations per second     systems with massive numbers of processors generally take one of two paths  in one approach  e g   in distributed computing   a large number of discrete computers  e g   laptops  distributed across a network  e g   the internet  devote some or all of their time to solving a common problem  each individual computer  client  receives and completes many small tasks  reporting the results to a central server which integrates the task results from all the clients into the overall solution    in another approach  a large number of dedicated processors are placed in close proximity to each other  e g  in a computer cluster   this saves considerable time moving data around and makes it possible for the processors to work together  rather than on separate tasks   for example in mesh and hypercube architectures     the use of multi core processors combined with centralization is an emerging trend  one can think of this as a small cluster  the multicore processor in a smartphone  tablet  laptop  etc   that both depends upon and contributes to the cloud       supercomputers play an important role in the field of computational science  and are used for a wide range of computationally intensive tasks in various fields  including quantum mechanics  weather forecasting  climate research  oil and gas exploration  molecular modeling  computing the structures and properties of chemical compounds  biological macromolecules  polymers  and crystals   and physical simulations  such as simulations of the early moments of the universe  airplane and spacecraft aerodynamics  the detonation of nuclear weapons  and nuclear fusion   throughout their history  they have been essential in the field of cryptanalysis              the history of supercomputing goes back to the     s  with the atlas at the university of manchester and a series of computers at control data corporation  cdc   designed by seymour cray  these used innovative designs and parallelism to achieve superior computational peak performance       the atlas was a joint venture between ferranti and the manchester university and was designed to operate at processing speeds approaching onexa microsecond per instruction  about onexa million instructions per second    the first atlas was officially commissioned on   december      as one of the world s first supercomputers   considered to be the most powerful computer in the world at that time by a considerable margin  and equivalent to four ibm     s       the cdc       released in       was designed by cray to be the fastest in the world by a large margin  cray switched from germanium to silicon transistors  which he ran very fast  solving the overheating problem by introducing refrigeration    given that the      outran all computers of the time by about    times  it was dubbed a supercomputer and defined the supercomputing market when one hundred computers were sold at    million each             cray left cdc in      to form his own company  cray research    four years after leaving cdc  cray delivered the   xa mhz cray   in       and it became one of the most successful supercomputers in history      the cray   released in      was an   processor liquid cooled computer and fluorinert was pumped through it as it operated  it performed at     gigaflops and was the world s fastest until            while the supercomputers of the     s used only a few processors  in the     s  machines with thousands of processors began to appear both in the united states and japan  setting new computational performance records  fujitsu s numerical wind tunnel supercomputer used     vector processors to gain the top spot in      with a peak speed of    xa gigaflops  gflops  per processor      the hitachi sr     obtained a peak performance of    xa gflops in      by using      processors connected via a fast three dimensional crossbar network        the intel paragon could have      to      intel i    processors in various configurations  and was ranked the fastest in the world in       the paragon was a mimd machine which connected processors via a high speed two dimensional mesh  allowing processes to execute on separate nodes  communicating via the message passing interface       approaches to supercomputer architecture have taken dramatic turns since the earliest systems were introduced in the     s  early supercomputer architectures pioneered by seymour cray relied on compact innovative designs and local parallelism to achieve superior computational peak performance    however  in time the demand for increased computational power ushered in the age of massively parallel systems     while the supercomputers of the     s used only a few processors  in the     s  machines with thousands of processors began to appear and by the end of the   th century  massively parallel supercomputers with tens of thousands of  off the shelf  processors were the norm  supercomputers of the   st century can use over         processors  some being graphic units  connected by fast connections    the connection machine cm   supercomputer is a massively parallel processing computer capable of many billions of arithmetic operations per second       throughout the decades  the management of heat density has remained a key issue for most centralized supercomputers        the large amount of heat generated by a system may also have other effects  e g  reducing the lifetime of other system components    there have been diverse approaches to heat management  from pumping fluorinert through the system  to a hybrid liquid air cooling system or air cooling with normal air conditioning temperatures         systems with a massive number of processors generally take one of two paths  in the grid computing approach  the processing power of a large number of computers  organised as distributed  diverse administrative domains  is opportunistically used whenever a computer is available   in another approach  a large number of processors are used in close proximity to each other  e g  in a computer cluster  in such a centralized massively parallel system the speed and flexibility of the interconnect becomes very important and modern supercomputers have used various approaches ranging from enhanced infiniband systems to three dimensional torus interconnects      the use of multi core processors combined with centralization is an emerging direction  e g  as in the cyclops   system       as the price  performance and energy efficiency of general purpose graphic processors  gpgpus  have improved    a number of petaflop supercomputers such as tianhe i and nebulae have started to rely on them    however  other systems such as the k computer continue to use conventional processors such as sparc based designs and the overall applicability of gpgpus in general purpose high performance computing applications has been the subject of debate  in that while a gpgpu may be tuned to score well on specific benchmarks  its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it    however  gpus are gaining ground and in      the jaguar supercomputer was transformed into titan by retrofitting cpus with gpus           high performance computers have an expected life cycle of about three years       a number of  special purpose  systems have been designed  dedicated to a single problem  this allows the use of specially programmed fpga chips or even custom vlsi chips  allowing better price performance ratios by sacrificing generality  examples of special purpose supercomputers include belle    deep blue    and hydra    for playing chess  gravity pipe for astrophysics    mdgrape   for protein structure computation molecular dynamics   and deep crack    for breaking the des cipher     a typical supercomputer consumes large amounts of electrical power  almost all of which is converted into heat  requiring cooling  for example  tianhe  a consumes     xa megawatts of electricity    the cost to power and cool the system can be significant  e g   xa mw at       kwh is      an hour or about      million per year     heat management is a major issue in complex electronic devices  and affects powerful computer systems in various ways    the thermal design power and cpu power dissipation issues in supercomputing surpass those of traditional computer cooling technologies  the supercomputing awards for green computing reflect this issue           the packing of thousands of processors together inevitably generates significant amounts of heat density that need to be dealt with  the cray   was liquid cooled  and used a fluorinert  cooling waterfall  which was forced through the modules under pressure    however  the submerged liquid cooling approach was not practical for the multi cabinet systems based on off the shelf processors  and in system x a special cooling system that combined air conditioning with liquid cooling was developed in conjunction with the liebert company       in the blue gene system  ibm deliberately used low power processors to deal with heat density    on the other hand  the ibm power      released in       has closely packed elements that require water cooling    the ibm aquasar system  on the other hand uses hot water cooling to achieve energy efficiency  the water being used to heat buildings as well         the energy efficiency of computer systems is generally measured in terms of  flops per watt   in       ibm s roadrunner operated at     xa mflops w      in november       the blue gene q reached      xa mflops w      in june      the top   spots on the green     list were occupied by blue gene machines in new york  one achieving     xa mflops w  with the degima cluster in nagasaki placing third with     xa mflops w       because copper wires can transfer energy into a supercomputer with much higher power densities than forced air or circulating refrigerants can remove waste heat    the ability of the cooling systems to remove waste heat is a limiting factor      as of     update  many existing supercomputers have more infrastructure capacity than the actual peak demand of the machine   people conservatively designed the power and cooling infrastructure to handle more than the theoretical peak electrical power consumed by the supercomputer  designs for future supercomputers are power limited   the thermal design power of the supercomputer as a whole  the amount that the power and cooling infrastructure can handle  is somewhat more than the expected normal power consumption  but less than the theoretical peak power consumption of the electronic hardware           since the end of the   th century  supercomputer operating systems have undergone major transformations  based on the changes in supercomputer architecture    while early operating systems were custom tailored to each supercomputer to gain speed  the trend has been to move away from in house operating systems to the adaptation of generic software such as linux       since modern massively parallel supercomputers typically separate computations from other services by using multiple types of nodes  they usually run different operating systems on different nodes  e g  using a small and efficient lightweight kernel such as cnk or cnl on compute nodes  but a larger system such as a linux derivative on server and i o nodes           while in a traditional multi user computer system job scheduling is  in effect  a tasking problem for processing and peripheral resources  in a massively parallel system  the job management system needs to manage the allocation of both computational and communication resources  as well as gracefully deal with inevitable hardware failures when tens of thousands of processors are present       although most modern supercomputers use the linux operating system  each manufacturer has its own specific linux derivative  and no industry standard exists  partly due to the fact that the differences in hardware architectures require changes to optimize the operating system to each hardware design         the parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed  software tools for distributed processing include standard apis such as mpi and pvm  vtl  and open source based software solutions such as beowulf     in the most common scenario  environments such as pvm and mpi for loosely connected clusters and openmp for tightly coordinated shared memory machines are used  significant effort is required to optimize an algorithm for the interconnect characteristics of the machine it will be run on  the aim is to prevent any of the cpus from wasting time waiting on data from other nodes  gpgpus have hundreds of processor cores and are programmed using programming models such as cuda     moreover  it is quite difficult to debug and test parallel programs  special techniques need to be used for testing and debugging such applications     opportunistic supercomputing is a form of networked grid computing whereby a  super virtual computer  of many loosely coupled volunteer computing machines performs very large computing tasks  grid computing has been applied to a number of large scale embarrassingly parallel problems that require supercomputing performance scales  however  basic grid and cloud computing approaches that rely on volunteer computing can not handle traditional supercomputing tasks such as fluid dynamic simulations     the fastest grid computing system is the distributed computing project folding home  f h reported     xa pflops of x   processing power as of june     update  of this      xa pflops are contributed by clients running on various gpus  and the rest from various cpu systems       the boinc platform hosts a number of distributed computing projects  as of may     update  boinc recorded a processing power of over    xa pflops through over         active computers on the network   the most active project  measured by computational power   milkyway home  reports processing power of over    xa teraflops  tflops  through over        active computers       as of may     update  gimps s distributed mersenne prime search currently achieves about   xa tflops through over        registered computers    the internet primenet server supports gimps s grid computing approach  one of the earliest and most successful grid computing projects  since          quasi opportunistic supercomputing is a form of distributed computing whereby the  super virtual computer  of a large number of networked geographically disperse computers performs computing tasks that demand huge processing power    quasi opportunistic supercomputing aims to provide a higher quality of service than opportunistic grid computing by achieving more control over the assignment of tasks to distributed resources and the use of intelligence about the availability and reliability of individual systems within the supercomputing network  however  quasi opportunistic distributed execution of demanding parallel computing software in grids should be achieved through implementation of grid wise allocation agreements  co allocation subsystems  communication topology aware allocation mechanisms  fault tolerant message passing libraries and data pre conditioning       supercomputers generally aim for the maximum in capability computing rather than capacity computing  capability computing is typically thought of as using the maximum computing power to solve a single large problem in the shortest amount of time  often a capability system is able to solve a problem of a size or complexity that no other computer can  e g  a very complex weather simulation application       capacity computing  in contrast  is typically thought of as using efficient cost effective computing power to solve a small number of somewhat large problems or a large number of small problems    architectures that lend themselves to supporting many users for routine everyday tasks may have a lot of capacity  but are not typically considered supercomputers  given that they do not solve a single very complex problem       in general  the speed of supercomputers is measured and benchmarked in  flops   floating point operations per second   and not in terms of  mips   million instructions per second   as is the case with general purpose computers    these measurements are commonly used with an si prefix such as tera   combined into the shorthand  tflops        flops  pronounced teraflops   or peta   combined into the shorthand  pflops        flops  pronounced petaflops    petascale  supercomputers can process one quadrillion              trillion  flops  exascale is computing performance in the exaflops  eflops  range  an eflops is one quintillion        flops  one million tflops      no single number can reflect the overall performance of a computer system  yet the goal of the linpack benchmark is to approximate how fast the computer solves numerical problems and it is widely used in the industry    the flops measurement is either quoted based on the theoretical floating point performance of a processor  derived from manufacturer s processor specifications and shown as  rpeak  in the top    lists  which is generally unachievable when running real workloads  or the achievable throughput  derived from the linpack benchmarks and shown as  rmax  in the top    list  the linpack benchmark typically performs lu decomposition of a large matrix  the linpack performance gives some indication of performance for some real world problems  but does not necessarily match the processing requirements of many other supercomputer workloads  which for example may require more memory bandwidth  or may require better integer computing performance  or may need a high performance i o system to achieve high levels of performance       since       the fastest supercomputers have been ranked on the top    list according to their linpack benchmark results  the list does not claim to be unbiased or definitive  but it is a widely cited current definition of the  fastest  supercomputer available at any given time     this is a recent list of the computers which appeared at the top of the top    list    and the  peak speed  is given as the  rmax  rating  for more historical data see history of supercomputing     sourcexa   top       the stages of supercomputer application may be summarized in the following table     the ibm blue gene p computer has been used to simulate a number of artificial neurons equivalent to approximately one percent of a human cerebral cortex  containing     billion neurons with approximately   trillion connections  the same research group also succeeded in using a supercomputer to simulate a number of artificial neurons equivalent to the entirety of a rat s brain       modern day weather forecasting also relies on supercomputers  the national oceanic and atmospheric administration uses supercomputers to crunch hundreds of millions of observations to help make weather forecasts more accurate       in       the challenges and difficulties in pushing the envelope in supercomputing were underscored by ibm s abandonment of the blue waters petascale project       given the current speed of progress  industry experts estimate that supercomputers will reach  xa eflops        one quintillion flops  by       in china industry experts estimate machines will start reaching       petaflop performance by         using the intel mic multi core processor architecture  which is intel s response to gpu systems  sgi plans to achieve a     fold increase in performance by       in order to achieve one exaflops  samples of mic chips with    cores  which combine vector processing units with standard cpu  have become available    the indian government has also stated ambitions for an exaflops range supercomputer  which they hope to complete by         in november       it was reported that india is working on the fastest supercomputer ever which is set to work at    xa eflops       erik p  debenedictis of sandia national laboratories theorizes that a zettaflops        one sextillion flops  computer is required to accomplish full weather modeling  which could cover a two week time span accurately   not in citation given such systems might be built around         