 in telecommunications  a communication protocol is a system of digital rules for data exchange within or between computers     communicating systems use well defined formats  protocol  for exchanging messages  each message has an exact meaning intended to elicit a response from a range of possible responses pre determined for that particular situation  thus  a protocol must define the syntax  semantics  and synchronization of communication  the specified behavior is typically independent of how it is to be implemented  a protocol can therefore be implemented as hardware  software  or both  communication protocols have to be agreed upon by the parties involved   to reach agreement  a protocol may be developed into a technical standard  a programming language describes the same for computations  so there is a close analogy between protocols and programming languages  protocols are to communications as programming languages are to computations              the information exchanged devices through a network  or other media is governed by rules and conventions that can be set out in technical specifications called communication protocol standards  the nature of a communication  the actual data exchanged and any state dependent behaviors  is defined by its specification     in digital computing systems  the rules can be expressed by algorithms and data structures  expressing the algorithms in a portable programming language makes the protocol software operating system independent     operating systems usually contain of a set of cooperating processes that manipulate shared data to communicate with each other  this communication is governed by well understood protocols  which can be embedded in the process code itself       in contrast  because there is no common memory  communicating systems have to communicate with each other using a shared transmission medium  transmission is not necessarily reliable  and individual systems may use different hardware and or operating systems     to implement a networking protocol  the protocol software modules are interfaced with a framework implemented on the machine s operating system  this framework implements the networking functionality of the operating system   the best known frameworks are the tcp ip model and the osi model     at the time the internet was developed  layering had proven to be a successful design approach for both compiler and operating system design and  given the similarities between programming languages and communication protocols  layering was applied to the protocols as well   this gave rise to the concept of layered protocols which nowadays forms the basis of protocol design      systems typically do not use a single protocol to handle a transmission  instead they use a set of cooperating protocols  sometimes called a protocol family or protocol suite   some of the best known protocol suites include  ipx spx  x     ax     appletalk and tcp ip     the protocols can be arranged based on functionality in groups  for instance there is a group of transport protocols  the functionalities are mapped onto the layers  each layer solving a distinct class of problems relating to  for instance  application   transport   internet  and network interface functions   to transmit a message  a protocol has to be selected from each layer  so some sort of multiplexing demultiplexing takes place  the selection of the next protocol is accomplished by extending the message with a protocol selector for each layer       messages are sent and received on communicating systems to establish communications  protocols should therefore specify rules governing the transmission  in general  much of the following should be addressed       getting the data across a network is only part of the problem for a protocol  the data received has to be evaluated in the context of the progress of the conversation  so a protocol has to specify rules describing the context  these kind of rules are said to express the syntax of the communications  other rules determine whether the data is meaningful for the context in which the exchange takes place  these kind of rules are said to express the semantics of the communications     formal ways for describing the syntax of the communications are abstract syntax notation one  an iso standard  or augmented backus naur form  an ietf standard      finite state machine models     and communicating finite state machines   are used to formally describe the possible interactions of the protocol     this analogy has important consequences for both the design and the development of protocols  one has to consider the fact that algorithms  programs and protocols are just different ways of describing expected behavior of interacting objects  a familiar example of a protocolling language is the html language used to describe web pages which are the actual web protocols     in programming languages the association of identifiers to a value is termed a definition  program text is structured using block constructs and definitions can be local to a block  the localized association of an identifier to a value established by a definition is termed a binding and the region of program text in which a binding is effective is known as its scope    the computational state is kept using two components  the environment  used as a record of identifier bindings  and the store  which is used as a record of the effects of assignments       in communications  message values are transferred using transmission media  by analogy  the equivalent of a store would be a collection of transmission media  instead of a collection of memory locations  a valid assignment in a protocol  as an analog of programming language  could be ethernet   message    meaning a message is to be broadcast on the local ethernet non a transmission medium there can be many receivers  for instance a mac address identifies an ether network card on the transmission medium  the  ether    in our imaginary protocol  the assignment ethernetmac address  message value could therefore make sense       by extending the assignment statement of an existing programming language with the semantics described  a protocolling language could easily be imagined     operating systems provide reliable communication and synchronization facilities for communicating objects confined to the same system by means of system libraries  a programmer using a general purpose programming language  like c or ada  can use the routines in the libraries to implement a protocol  instead of using a dedicated protocolling language     despite their numbers  networking protocols show little variety  because all networking protocols use the same underlying principles and concepts  in the same way  so  the use of a general purpose programming language would yield a large number of applications only differing in the details    a suitably defined  dedicated  protocolling language would therefore have little syntax  perhaps just enough to specify some parameters or optional modes of operation  because its virtual machine would have incorporated all possible principles and concepts making the virtual machine itself a universal protocol  the protocolling language would have some syntax and a lot of semantics describing this universal protocol and would therefore in effect be a protocol  hardly differing from this universal networking protocol  in this  networking  context a protocol is a language     the notion of a universal networking protocol provides a rationale for standardization of networking protocols  assuming the existence of a universal networking protocol  development of protocol standards using a consensus model  the agreement of a group of experts  might be a viable way to coordinate protocol design efforts     networking protocols operate in very heterogeneous environments consisting of very different network technologies and a  possibly  very rich set of applications  so a single universal protocol would be very hard to design and implement correctly  instead  the ietf decided to reduce complexity by assuming a relatively simple network architecture allowing decomposition of the single universal networking protocol into two generic protocols  tcp and ip  and two classes of specific protocols  one dealing with the low level network details and one dealing with the high level details of common network applications  remote login  file transfer  email and web browsing   iso choose a similar but more general path  allowing other network architectures  to standardize protocols     communicating systems operate in parallel  the programming tools and techniques for dealing with parallel processes are collectively called concurrent programming  concurrent programming only deals with the synchronization of communication  the syntax and semantics of the communication governed by a low level protocol usually have modest complexity  so they can be coded with relative ease  high level protocols with relatively large complexity could however merit the implementation of language interpreters  an example of the latter case is the html language nconcurrent programming has traditionally been a topic in operating systems theory texts    formal verification seems indispensable  because concurrent programs are notorious for the hidden and sophisticated bugs they contain    a mathematical approach to the study of concurrency and communication is referred to as communicating sequential processes  csp     concurrency can also be modelled using finite state machines like mealy and moore machines  mealy and moore machines are in use as design tools in digital electronics systems  which we encounter in the form of hardware used in telecommunications or electronic devices in general       this kind of design can be a bit of a challenge to say the least  so it is important to keep things simple  for the internet protocols  in particular and in retrospect  this meant a basis for protocol design was needed to allow decomposition of protocols into much simpler  cooperating protocols     systems do not use a single protocol to handle a transmission  instead they use a set of cooperating protocols  sometimes called a protocol family or protocol suite   to cooperate the protocols have to communicate with each other  so some kind of conceptual framework is needed to make this communication possible  also note that software is needed to implement both the  xfer mechanism  and a protocol  no protocol  no communication      in literature there are numerous references to the analogies between computer communication and programming  by analogy we could say that the aforementioned  xfer mechanism  is comparable to a cpu  a  xfer mechanism  performs communications and a cpu performs computations and the  framework  introduces something that allows the protocols to be designed independent of one another by providing separate execution environments for them  furthermore  it is repeatedly stated that protocols are to computer communication what programming languages are to computation         the communication protocols in use on the internet are designed to function in very complex and diverse settings  to ease design  communication protocols are structured using a layering scheme as a basis  instead of using a single universal protocol to handle all transmission tasks  a set of cooperating protocols fitting the layering scheme is used    the layering scheme in use on the internet is called the tcp ip model  the actual protocols are collectively called the internet protocol suite  the group responsible for this design is called the internet engineering task force  ietf      typically  a hardware delivery mechanism layer is used to build a connectionless packet delivery system on top of which a reliable transport layer is built  on top of which is the application software  layers below and above these can be defined  and protocols are very often stacked to give tunnelling  for example the internet protocol can be tunnelled across an atm network protocol to provide connectivity by layering the internet protocol on top of the atm protocol transport layer     the number of layers of a layering scheme and the way the layers are defined can have a drastic impact on the protocols involved  this is where the analogies come into play for the tcp ip model  because the designers of tcp ip employed the same techniques used to conquer the complexity of programming language compilers  design by analogy  in the implementation of its protocols and its layering scheme       protocol layering now forms the basis of protocol design   it allows the decomposition of single  complex protocols into simpler  cooperating protocols  but it is also a functional decomposition  because each protocol belongs to a functional class  called a protocol layer    the protocol layers each solve a distinct class of communication problems  the internet protocol suite consists of the following layers  application   transport   internet  and network interface functions   together  the layers make up a layering scheme or model     in computations  we have algorithms and data  and in communications  we have protocols and messages  so the analog of a data flow diagram would be some kind of message flow diagram    to visualize protocol layering and protocol suites  a diagram of the message flows in and between two systems  a and b  is shown in figure       the systems both make use of the same protocol suite  the vertical flows  and protocols  are in system and the horizontal message flows  and protocols  are between systems  the message flows are governed by rules  and data formats specified by protocols  the blue lines therefore mark the boundaries of the  horizontal  protocol layers     the vertical protocols are not layered because they don t obey the protocol layering principle which states that a layered protocol is designed so that layer n at the destination receives exactly the same object sent by layer n at the source  the horizontal protocols are layered protocols and all belong to the protocol suite  layered protocols allow the protocol designer to concentrate on one layer at a time  without worrying about how other layers perform       the vertical protocols need not be the same protocols on both systems  but they have to satisfy some minimal assumptions to ensure the protocol layering principle holds for the layered protocols  this can be achieved using a technique called encapsulation       usually  a message or a stream of data is divided into small pieces  called messages or streams  packets  ip datagrams or network frames depending on the layer in which the pieces are to be transmitted  the pieces contain a header area and a data area  the data in the header area identifies the source and the destination on the network of the packet  the protocol  and other data meaningful to the protocol like crc s of the data to be sent  data length  and a timestamp         the rule enforced by the vertical protocols is that the pieces for transmission are to be encapsulated in the data area of all lower protocols on the sending side and the reverse is to happen on the receiving side  the result is that at the lowest level the piece looks like this   header  header  header  data  and in the layer directly above it   header  header  data  and in the top layer   header  data   both on the sending and receiving side  this rule therefore ensures that the protocol layering principle holds and effectively virtualizes all but the lowest transmission lines  so for this reason some message flows are coloured red in figure       to ensure both sides use the same protocol  the pieces also carry data identifying the protocol in their header     the design of the protocol layering and the network  or internet  architecture are interrelated  so one cannot be designed without the other    some of the more important features in this respect of the internet architecture and the network services it provides are described next     having established the protocol layering and the protocols  the protocol designer can now resume with the software design  the software has a layered organization and its relationship with protocol layering is visualized in figure       the software modules implementing the protocols are represented by cubes  the information flow between the modules is represented by arrows  the  top two horizontal  red arrows are virtual  the blue lines mark the layer boundaries     to send a message on system a  the top module interacts with the module directly below it and hands over the message to be encapsulated  this module reacts by encapsulating the message in its own data area and filling in its header data in accordance with the protocol it implements and interacts with the module below it by handing over this newly formed message whenever appropriate  the bottom module directly interacts with the bottom module of system b  so the message is sent across  on the receiving system b the reverse happens  so ultimately  and assuming there were no transmission errors or protocol violations etc   the message gets delivered in its original form to the topmodule of system b   non protocol errors  a receiving module discards the piece it has received and reports back the error condition to the original source of the piece on the same layer by handing the error message down or in case of the bottom module sending it across   nthe division of the message or stream of data into pieces and the subsequent reassembly are handled in the layer that introduced the division reassembly  the reassembly is done at the destination  i e  not on any intermediate routers        tcp ip software is organized in four layers       program translation has been divided into four subproblems  compiler  assembler  link editor  and loader  as a result  the translation software is layered as well  allowing the software layers to be designed independently  noting that the ways to conquer the complexity of program translation could readily be applied to protocols because of the analogy between programming languages and protocols  the designers of the tcp ip protocol suite were keen on imposing the same layering on the software framework  this can be seen in the tcp ip layering by considering the translation of a pascal program  message  that is compiled  function of the application layer  into an assembler program that is assembled  function of the transport layer  to object code  pieces  that is linked  function of the internet layer  together with library object code  routing table  by the link editor  producing relocatable machine code  datagram  that is passed to the loader which fills in the memory locations  ethernet addresses  to produce executable code  network frame  to be loaded  function of the network interface layer  into physical memory  transmission medium   to show just how closely the analogy fits  the terms between parentheses in the previous sentence denote the relevant analogs and the terms written cursively denote data representations  program translation forms a linear sequence  because each layer s output is passed as input to the next layer  furthermore  the translation process involves multiple data representations  we see the same thing happening in protocol software where multiple protocols define the data representations of the data passed between the software modules       the network interface layer uses physical addresses and all the other layers only use ip addresses  the boundary between network interface layer and internet layer is called the high level protocol address boundary    the modules below the application layer are generally considered part of the operating system  passing data between these modules is much less expensive than passing data between an application program and the transport layer  the boundary between application layer and transport layer is called the operating system boundary       strictly adhering to a layered model  a practice known as strict layering  is not always the best approach to networking    strict layering  can have a serious impact on the performance of the implementation  so there is at least a trade off between simplicity and performance    another  perhaps more important point can be shown by considering the fact that some of the protocols in the internet protocol suite cannot be expressed using the tcp ip model  in other words some of the protocols behave in ways not described by the model    to improve on the model  an offending protocol could  perhaps be split up into two protocols  at the cost of one or two extra layers  but there is a hidden caveat  because the model is also used to provide a conceptual view on the suite for the intended users  there is a trade off to be made here between preciseness for the designer and clarity for the intended user       for communication to take place  protocols have to be agreed upon  recall that in digital computing systems  the rules can be expressed by algorithms and datastructures  raising the opportunity for hardware independence  expressing the algorithms in a portable programming language  makes the protocol software operating system independent  the source code could be considered a protocol specification  this form of specification  however is not suitable for the parties involved     for one thing  this would enforce a source on all parties and for another  proprietary software producers would not accept this  by describing the software interfaces of the modules on paper and agreeing on the interfaces  implementers are free to do it their way  this is referred to as source independence  by specifying the algorithms on paper and detailing hardware dependencies in an unambiguous way  a paper draft is created  that when adhered to and published  ensures interoperability between software and hardware     such a paper draft can be developed into a protocol standard by getting the approval of a standards organization  to get the approval the paper draft needs to enter and successfully complete the standardization process  this activity is referred to as protocol development  the members of the standards organization agree to adhere to the standard on a voluntary basis  often the members are in control of large market shares relevant to the protocol and in many cases  standards are enforced by law or the government  because they are thought to serve an important public interest  so getting approval can be very important for the protocol     it should be noted though that in some cases protocol standards are not sufficient to gain widespread acceptance i e  sometimes the source code needs to be disclosed and enforced by law or the government in the interest of the public     the need for protocol standards can be shown by looking at what happened to the bi sync protocol  bsc  invented by ibm  bsc is an early link level protocol used to connect two separate nodes  it was originally not intended to be used in a multinode network  but doing so revealed several deficiencies of the protocol  in the absence of standardization  manufacturers and organizations felt free to  enhance  the protocol  creating incompatible versions on their networks  in some cases  this was deliberately done to discourage users from using equipment from other manufacturers  there are more than    variants of the original bi sync protocol  one can assume  that a standard would have prevented at least some of this from happening      in some cases  protocols gain market dominance without going through a standardization process  such protocols are referred to as de facto standards  de facto standards are common in emerging markets  niche markets  or markets that are monopolized  or oligopolized   they can hold a market in a very negative grip  especially when used to scare away competition  from a historical perspective  standardization should be seen as a measure to counteract the ill effects of de facto standards  positive exceptions exist  a  de facto standard  operating system like gnu linux does not have this negative grip on its market  because the sources are published and maintained in an open way  thus inviting competition  standardization is therefore not the only solution for open systems interconnection     some of the standards organizations of relevance for communication protocols are the international organization for standardization  iso   the international telecommunication union  itu   the institute of electrical and electronics engineers  ieee   and the internet engineering task force  ietf   the ietf maintains the protocols in use on the internet  the ieee controls many software and hardware protocols in the electronics industry for commercial and consumer devices  the itu is an umbrella organization of telecommunication engineers designing the public switched telephone network  pstn   as well as many radio communication systems  for marine electronics the nmea standards are used  the world wide web consortium  w c  produces protocols and standards for web technologies     international standards organizations are supposed to be more impartial than local organizations with a national or commercial self interest to consider  standards organizations also do research and development for standards of the future  in practice  the standards organizations mentioned  cooperate closely with each other       the standardization process starts off with iso commissioning a sub committee workgroup  the workgroup issues working drafts and discussion documents to interested parties  including other standards bodies  in order to provoke discussion and comments  this will generate a lot of questions  much discussion and usually some disagreement on what the standard should provide and if it can satisfy all needs  usually not   all conflicting views should be taken into account  often by way of compromise  to progress to a draft proposal of the working group     the draft proposal is discussed by the member countries  standard bodies and other organizations within each country  comments and suggestions are collated and national views will be formulated  before the members of iso vote on the proposal  if rejected  the draft proposal has to consider the objections and counter proposals to create a new draft proposal for another vote  after a lot of feedback  modification  and compromise the proposal reaches the status of a draft international standard  and ultimately an international standard     the process normally takes several years to complete  the original paper draft created by the designer will differ substantially from the standard  and will contain some of the following  features      international standards are reissued periodically to handle the deficiencies and reflect changing views on the subject       a lesson learned from arpanet  the predecessor of the internet  is that standardization of protocols is not enough  because protocols also need a framework to operate  it is therefore important to develop a general purpose  future proof framework suitable for structured protocols  such as layered protocols  and their standardization  this would prevent protocol standards with overlapping functionality and would allow clear definition of the responsibilities of a protocol at the different levels  layers     this gave rise to the iso open systems interconnection reference model  rm osi   which is used as a framework for the design of standard protocols and services conforming to the various layer specifications       in the osi model  communicating systems are assumed to be connected by an underlying physical medium providing a basic  and unspecified  transmission mechanism  the layers above it are numbered  from one to seven   the nth layer is referred to as  n  layer  each layer provides service to the layer above it  or at the top to the application process  using the services of the layer immediately below it  the layers communicate with each other by means of an interface  called a service access point  corresponding layers at each system are called peer entities  to communicate  two peer entities at a given layer use an  n  protocol  which is implemented by using services of the  n    layer  when systems are not directly connected  intermediate peer entities  called relays  are used  an address uniquely identifies a service access point  the address naming domains need not be restricted to one layer  so it is possible to use just one naming domain for all layers    for each layer there are two types of standards  protocol standards defining how peer entities at a given layer communicate  and service standards defining how a given layer communicates with the layer above it     in the original version of rm osi  the layers and their functionality are  from highest to lowest layer      in contrast to the tcp ip layering scheme  which assumes a connectionless network  rm osi assumed a connection oriented network  connection oriented networks are more suitable for wide area networks and connectionless networks are more suitable for local area networks  using connections to communicate implies some form of session and  virtual  circuits  hence the  in the tcp ip model lacking  session layer  the constituent members of iso were mostly concerned with wide area networks  so development of rm osi concentrated on connection oriented networks and connectionless networks were only mentioned in an addendum to rm osi    at the time  the ietf had to cope with this and the fact that the internet needed protocols which simply were not there  as a result the ietf developed its own standardization process based on  rough consensus and running code        the standardization process is described by rfc         nowadays  the ietf has become a standards organization for the protocols in use on the internet  rm osi has extended its model to include connectionless services and because of this  both tcp and ip could be developed into international standards     classification schemes for protocols usually focus on domain of use and function  as an example of domain of use  connection oriented protocols and connectionless protocols are used on connection oriented networks and connectionless networks respectively  for an example of function consider a tunneling protocol  which is used to encapsulate packets in a high level protocol  so the packets can be passed across a transport system using the high level protocol     a layering scheme combines both function and domain of use  the dominant layering schemes are the ones proposed by the ietf and by iso  despite the fact that the underlying assumptions of the layering schemes are different enough to warrant distinguishing the two  it is a common practice to compare the two by relating common protocols to the layers of the two schemes    for an example of this practice see  list of network protocols     the layering scheme from the ietf is called internet layering or tcp ip layering  the functionality of the layers has been described in the section on software layering and an overview of protocols using this scheme is given in the article on internet protocols     the layering scheme from iso is called the osi model or iso layering  the functionality of the layers has been described in the section on the future of standardization and an overview of protocols using this scheme is given in the article on osi protocols     the internet protocol is used in concert with other protocols within the internet protocol suite  prominent members of which include     other instances of high level interaction protocols are  