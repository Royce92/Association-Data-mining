 in computer networking  a reliable protocol provides reliability properties with respect to the delivery of data to the intended recipient s   as opposed to an unreliable protocol  which does not provide notifications to the sender as to the delivery of transmitted data  the term  reliable  is a synonym for assured  which is the term used by the itu and atm forum in the context of the atm service specific coordination function  for example for transparent assured delivery with aal         reliable protocols typically incur more overhead than unreliable protocols  and as a result  function more slowly and with less scalability  this often is not an issue for unicast protocols  but it may become a problem for reliable multicast protocols     tcp  the main protocol used on the internet  is a reliable unicast protocol  udp  often used in computer games or in other situations where speed is an issue and the loss of a little data is not as important because of the transitory nature of the data  is an unreliable protocol     often  a reliable unicast protocol is also connection oriented  for example  tcp is connection oriented  with the virtual circuit id consisting of source and destination ip addresses and port numbers  some unreliable protocols are connection oriented as well  these include atm and frame relay  there are also reliable connectionless protocols  such as ax    when it passes data in i frames  but this combination occurs rarely  reliable connectionless is uncommon in commercial and academic networks             when the arpanet pioneered packet switching  it provided a reliable packet delivery procedure to its connected hosts via its      interface  a host computer simply arranged the data in the correct packet format  inserted the address of the destination host computer  and sent the message across the interface to its connected interface message processor  once the message was delivered to the destination host  an acknowledgement was delivered to the sending host  if the network could not deliver the message  it would send an error message back to the sending host     meanwhile  the developers of cyclades and of alohanet demonstrated that it was possible to build an effective computer network without providing reliable packet transmission  this lesson was later embraced by the designers of ethernet     if a network does not guarantee packet delivery  then it becomes the host s responsibility to provide reliability by detecting and retransmitting lost packets  subsequent experience on the arpanet indicated that the network itself could not reliably detect all packet delivery failures  and this pushed responsibility for error detection onto the sending host in any case  this led to the development of the end to end principle  which is one of the internet s fundamental design assumptions     a reliable service is one that notifies the user if delivery fails  while an  unreliable  one does not notify the user if delivery fails  for example  ip provides an unreliable service  together  tcp and ip provide a reliable service  whereas udp and ip provide an unreliable one  all these protocols use packets  but udp packets are generally called datagrams      in the context of distributed protocols  reliability properties specify the guarantees that the protocol provides with respect to the delivery of messages to the intended recipient s      an example of a reliability property for a unicast protocol is  at least once   i e  at least one copy of the message is guaranteed to be delivered to the recipient     reliability properties for multicast protocols can be expressed on a per recipient basis  simple reliability properties   or they may relate the fact of delivery or the order of delivery among the different recipients  strong reliability properties      in the context of multicast protocols  strong reliability properties express the guarantees that the protocol provides with respect to the delivery of messages to different recipients     an example of a strong reliability property is last copy recall  meaning that as long as at least a single copy of a message remains available at any of the recipients  every other recipient that does not fail eventually also receives a copy  strong reliability properties such as this one typically require that messages are retransmitted or forwarded among the recipients     an example of a reliability property stronger than last copy recall is atomicity  the property states that if at least a single copy of a message has been delivered to a recipient  all other recipients will eventually receive a copy of the message  in other words  each message is always delivered to either all or none of the recipients     one of the most complex strong reliability properties is virtual synchrony     strong reliability properties are offered by group communication systems  gcs  such as is is  appia framework  spread  jgroups or quicksilver scalable multicast  the quicksilver properties framework is a flexible platform that allows strong reliability properties to be expressed in a purely declarative manner  using a simple rule based language  and automatically translated into a hierarchical protocol     there is  however  a problem with the definition of reliability as  delivery or notification of failure  in real time computing  in such systems  failure to deliver the real time data will adversely affect the performance of the systems  and some systems  e g  safety critical  safety involved  and some secure mission critical systems  must be proved to perform at some specified minimum level  this  in turn  requires that there be a specified minimum reliability for the delivery of the critical data  hence  the notification of failure does not negate or ameliorate delivery failure by the real time system s transport layer     in hard and firm real time systems the data has to be delivered within a deadline  i e  data that is delivered late is valueless  in hard real time systems all data must be delivered within its deadline or it is considered a system failure  in firm real time systems  there is some acceptable probability that data will be not be delivered or may be delivered late   these being equivalent       there are a number of protocols that are capable of meeting real time requirements for reliable delivery and timeliness  at least for firm real time systems  due to the inevitable and unavoidable losses from  e g   the physical layer bit error rates      mil std     b and stanag      are well known examples of such timely and reliable protocols for avioninc data buses  mil      uses a   mbit s shared media for the transmission of data and the control of these transmissions  and is widely used in federated military avionics systems  in which  each system has its own computers performing its own functions     it uses a bus controller  bc  to command the connected remote terminals  rts  to receive or transmit this data  the bc can therefore ensure that there will be no congestion  and transfers are always timely  the mil      protocol also allows for automatic retries that can still ensure timely delivery and increase the reliability above that of the physical layer  stanag       also known as efabus in its use on the eurofighter typhoon  is  in effect  a version of mil      augmented with a    mbit s shared media bus for data transfers  retaining the   mbit s shared media bus for control purposes     the asynchronous transfer mode  atm   the avionics full duplex switched ethernet  afdx   and time triggered ethernet  ttethernet  are examples of packet switched networks protocols where the timeliness and reliability of data transfers can be proved  afdx and ttethernet are also based on ieee       ethernet  though not entirely compatible with it     atm uses connection oriented virtual channels  vcs   which have fully deterministic paths through the network  and usage and network parameter control  upc npc   to limit the traffic on each vc separately  this allows the usage of the shared resources  switch buffers  in the network to be calculated from the parameters of the traffic to be carried  in advance  i e  at system design time  these can then be compared with the capacities of these resources to show that  given the constraints on the routes and the bandwidths of these connections  the resource used for these transfers will never be over subscribed  these transfers will therefore never be affected by congestion  and there will be no losses due to this effect  then  from the predicted maximum usages of the switch buffers  the maximum delay through the network can also be predicted  however  for the reliability and timeliness to be proved  and for the proofs to be tolerant of faults in and malicious actions by the equipment connected to the network  the calculations of these resource usages cannot be based on any parameters that are not actively enforced by the network  i e  they cannot be based on what the sources of the traffic are expected to do or on statistical analyses of the traffic characteristics  see network calculus       afdx uses frequency domain traffic policing or bandwidth allocation  that allows the traffic on each virtual link  vl  to be limited so that the requirements for shared resources can be predicted and congestion prevented so it can be proved not to affect the critical data   however  the techniques for predicting the resource requirements and proving that congestion is prevented are not part of the afdx standard     ttethernet provides the lowest possible latency in transferring data across such a network by using time domain control methods   each time triggered transfer is scheduled at a specific time  so that contention for shared resources is entirely controlled and thus the possibility of congestion is eliminated  the switches in the network enforce this timing to provide tolerance of faults in  and malicious actions on the part of  the other connected equipments  however   synchronized local clocks are the fundamental prerequisite for time triggered communication     this is because the sources of critical data will have to have the same view of time as the switch  in order that they can transmit at the correct time and the switch will see this as correct  this also requires that the sequence with which a critical transfer is scheduled has to be predictable to both source and switch  this  in turn  will limit the transmission schedule to a highly deterministic one  e g  the cyclic executive     however  low latency in transferring data over the bus or network does not necessarily translate into low transport delays between the application processes that source and sink this data  this is especially true where the transfers over the bus or network are cyclically scheduled  as is commonly the case with mil std     b and stanag       and necessarily so with afdx and ttethernet  but the application processes are asynchronous  e g  pre emptively scheduled  or only plesiosynchronous with this schedule  in which case  the maximum delay and jitter will be twice the update rate for the cyclic transfer  transfers wait up to the update interval between release and transmission and again wait up to the update interval between delivery and use      with both afdx and ttethernet  there are additional functions required of the interfaces to the network for the transmission of critical data  etc   that make it difficult to use standard ethernet interfaces  e g  afdx s bandwidth allocation gap control  and ttehernet s requirement for very close synchronization of the sources of time triggered data  other methods for control of the traffic in the network that would allow the use of such standard ieee       network interfaces is a subject of current research    