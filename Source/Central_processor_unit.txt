 a central processing unit  cpu  is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic  logical  control and input output  i o  operations specified by the instructions  the term has been used in the computer industry at least since the early     s   traditionally  the term  cpu  refers to a processor and its control unit  cu   distinguishing these core elements of a computer from external components such as main memory and i o circuitry      the form  design and implementation of cpus have changed over the course of their history  but their fundamental operation remains almost unchanged  principal components of a cpu include the arithmetic logic unit  alu  that performs arithmetic and logic operations  hardware registers that supply operands to the alu and store the results of alu operations  and a control unit that fetches instructions from memory and  executes  them by directing the coordinated operations of the alu  registers and other components     most modern cpus are microprocessors  meaning they are contained on a single integrated circuit  ic  chip  an ic that contains a cpu may also contain memory  peripheral interfaces  and other components of a computer  such integrated devices are variously called microcontrollers or systems on a chip  soc   some computers employ a multi core processor  which is a single chip containing two or more cpus called  cores   in that context  single chips are sometimes referred to as  sockets    array processors or vector processors have multiple processors that operate in parallel  with no unit considered central             computers such as the eniac had to be physically rewired to perform different tasks  which caused these machines to be called  fixed program computers    since the term  cpu  is generally defined as a device for software  computer program  execution  the earliest devices that could rightly be called cpus came with the advent of the stored program computer     the idea of a stored program computer was already present in the design of j  presper eckert and john william mauchly s eniac  but was initially omitted so that it could be finished sooner  on junexa           before eniac was made  mathematician john von neumann distributed the paper entitled first draft of a report on the edvac  it was the outline of a stored program computer that would eventually be completed in august        edvac was designed to perform a certain number of instructions  or operations  of various types  significantly  the programs written for edvac were to be stored in high speed computer memory rather than specified by the physical wiring of the computer  this overcame a severe limitation of eniac  which was the considerable time and effort required to reconfigure the computer to perform a new task  with von neumann s design  the program  or software  that edvac ran could be changed simply by changing the contents of the memory  edvac  however  was not the first stored program computer  the manchester small scale experimental machine  a small prototype stored program computer  ran its first program on    june       and the manchester mark   ran its first program during the night of       june          early cpus were custom designed as a part of a larger  sometimes one of a kind  computer  however  this method of designing custom cpus for a particular application has largely given way to the development of mass produced processors that are made for many purposes  this standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuitxa  ic   the ic has allowed increasingly complex cpus to be designed and manufactured to tolerances on the order of nanometers  both the miniaturization and standardization of cpus have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines  modern microprocessors appear in everything from automobiles to cell phones and children s toys     while von neumann is most often credited with the design of the stored program computer because of his design of edvac  others before him  such as konrad zuse  had suggested and implemented similar ideas  the so called harvard architecture of the harvard mark i  which was completed before edvac  also utilized a stored program design using punched paper tape rather than electronic memory  the key difference between the von neumann and harvard architectures is that the latter separates the storage and treatment of cpu instructions and data  while the former uses the same memory space for both  most modern cpus are primarily von neumann in design  but cpus with the harvard architecture are seen as well  especially in embedded applications  for instance  the atmel avr microcontrollers are harvard architecture processors     relays and vacuum tubes  thermionic valves  were commonly used as switching elements  a useful computer requires thousands or tens of thousands of switching devices  the overall speed of a system is dependent on the speed of the switches  tube computers like edvac tended to average eight hours between failures  whereas relay computers like the  slower  but earlier  harvard mark i failed very rarely   in the end  tube based cpus became dominant because the significant speed advantages afforded generally outweighed the reliability problems  most of these early synchronous cpus ran at low clock rates compared to modern microelectronic designs  see below for a discussion of clock rate   clock signal frequencies ranging from     khz to  xa mhz were very common at this time  limited largely by the speed of the switching devices they were built with     the design complexity of cpus increased as various technologies facilitated building smaller and more reliable electronic devices  the first such improvement came with the advent of the transistor  transistorized cpus during the     s and     s no longer had to be built out of bulky  unreliable  and fragile switching elements like vacuum tubes and electrical relays  with this improvement more complex and reliable cpus were built onto one or several printed circuit boards containing discrete  individual  components     during this period  a method of manufacturing many interconnected transistors in a compact space was developed  the integrated circuit  ic  allowed a large number of transistors to be manufactured on a single semiconductor based die  or  chip   at first only very basic non specialized digital circuits such as nor gates were miniaturized into ics  cpus based upon these  building block  ics are generally referred to as  small scale integration   ssi  devices  ssi ics  such as the ones used in the apollo guidance computer  usually contained up to a few score transistors  to build an entire cpu out of ssi ics required thousands of individual chips  but still consumed much less space and power than earlier discrete transistor designs  as microelectronic technology advanced  an increasing number of transistors were placed on ics  thus decreasing the quantity of individual ics needed for a complete cpu  msi and lsi  medium  and large scale integration  ics increased transistor counts to hundreds  and then thousands     in       ibm introduced its system     computer architecture that was used in a series of computers capable of running the same programs with different speed and performance  this was significant at a time when most electronic computers were incompatible with one another  even those made by the same manufacturer  to facilitate this improvement  ibm utilized the concept of a microprogram  often called  microcode    which still sees widespread usage in modern cpus   the system     architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the ibm zseries  in the same year         digital equipment corporation  dec  introduced another influential computer aimed at the scientific and research markets  the pdp    dec would later introduce the extremely popular pdp    line that originally was built with ssi ics but was eventually implemented with lsi components once these became practical  in stark contrast with its ssi and msi predecessors  the first lsi implementation of the pdp    contained a cpu composed of only four lsi integrated circuits      transistor based computers had several distinct advantages over their predecessors  aside from facilitating increased reliability and lower power consumption  transistors also allowed cpus to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay  thanks to both the increased reliability as well as the dramatically increased speed of the switching elements  which were almost exclusively transistors by this time   cpu clock rates in the tens of megahertz were obtained during this period  additionally while discrete transistor and ic cpus were in heavy usage  new high performance designs like simd  single instruction multiple data  vector processors began to appear  these early experimental designs later gave rise to the era of specialized supercomputers like those made by cray inc     in the     s the fundamental inventions by federico faggin  silicon gate mos ics with self aligned gates along with his new random logic design methodology  changed the design and implementation of cpus forever  since the introduction of the first commercially available microprocessor  the intel       in       and the first widely used microprocessor  the intel       in       this class of cpus has almost completely overtaken all other central processing unit implementation methods  mainframe and minicomputer manufacturers of the time launched proprietary ic development programs to upgrade their older computer architectures  and eventually produced instruction set compatible microprocessors that were backward compatible with their older hardware and software  combined with the advent and eventual success of the ubiquitous personal computer  the term cpu is now applied almost exclusivelya to microprocessors  several cpus  denoted  cores   can be combined in a single processing chip      previous generations of cpus were implemented as discrete components and numerous small integrated circuits  ics  on one or more circuit boards  microprocessors  on the other hand  are cpus manufactured on a very small number of ics  usually just one  the overall smaller cpu size  as a result of being implemented on a single die  means faster switching time because of physical factors like decreased gate parasitic capacitance  this has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz  additionally  as the ability to construct exceedingly small transistors on an ic has increased  the complexity and number of transistors in a single cpu has increased many fold  this widely observed trend is described by moore s law  which has proven to be a fairly accurate predictor of the growth of cpu  and other ic  complexity      while the complexity  size  construction  and general form of cpus have changed enormously since       it is notable that the basic design and function has not changed much at all  almost all common cpus today can be very accurately described as von neumann stored program machines b as the aforementioned moore s law continues to hold true   concerns have arisen about the limits of integrated circuit transistor technology  extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant  these newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer  as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von neumann model     the fundamental operation of most cpus  regardless of the physical form they take  is to execute a sequence of stored instructions called a program  the instructions are kept in some kind of computer memory  there are three steps that nearly all cpus use in their operation  fetch  decode  and execute     after the execution of an instruction  the entire process repeats  with the next instruction cycle normally fetching the next in sequence instruction because of the incremented value in the program counter  if a jump instruction was executed  the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally  in more complex cpus  multiple instructions can be fetched  decoded  and executed simultaneously  this section describes what is generally referred to as the  classic risc pipeline   which is quite common among the simple cpus used in many electronic devices  often called microcontroller   it largely ignores the important role of cpu cache  and therefore the access stage of the pipeline     some instructions manipulate the program counter rather than producing result data directly  such instructions are generally called  jumps  and facilitate program behavior like loops  conditional program execution  through the use of a conditional jump   and existence of functions c in some processors  some other instructions change the state of bits in a  flags  register  these flags can be used to influence how a program behaves  since they often indicate the outcome of various operations  for example  in such processors a  compare  instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal  one of these flags could then be used by a later jump instruction to determine program flow     the first step  fetch  involves retrieving an instruction  which is represented by a number or sequence of numbers  from program memory  the instruction s location  address  in program memory is determined by a program counter  pc   which stores a number that identifies the address of the next instruction to be fetched  after an instruction is fetched  the pc is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence d often  the instruction to be fetched must be retrieved from relatively slow memory  causing the cpu to stall while waiting for the instruction to be returned  this issue is largely addressed in modern processors by caches and pipeline architectures  see below      the instruction that the cpu fetches from memory determines what the cpu has to do  in the decode step  the instruction is broken up into parts that have significance to other portions of the cpu  the way in which the numerical instruction value is interpreted is defined by the cpu s instruction set architecture  isa  e often  one group of numbers in the instruction  called the opcode  indicates which operation to perform  the remaining parts of the number usually provide information required for that instruction  such as operands for an addition operation  such operands may be given as a constant value  called an immediate value   or as a place to locate a value  a register or a memory address  as determined by some addressing mode     in some cpu designs the instruction decoder is implemented as a hardwired  unchangeable circuit  in others  a microprogram is used to translate instructions into sets of cpu configuration signals that are applied sequentially over multiple clock pulses  in some cases the memory that stores the microprogram is rewritable  making it possible to change the way in which the cpu decodes instructions     after the fetch and decode steps  the execute step is performed  depending on the cpu architecture  this may consist of a single action or a sequence of actions  during each action  various parts of the cpu are electrically connected so they can perform all or part of the desired operation and then the action is completed  typically in response to a clock pulse  very often the results are written to an internal cpu register for quick access by subsequent instructions  in other cases results may be written to slower  but less expensive and higher capacity main memory     for example  if an addition instruction is to be executed  the arithmetic logic unit  alu  inputs are connected to a pair of operand sources  numbers to be summed   the alu is configured to perform an addition operation so that the sum of its operand inputs will appear at its output  and the alu output is connected to storage  e g   a register or memory  that will receive the sum  when the clock pulse occurs  the sum will be transferred to storage and  if the resulting sum is too large  i e   it is larger than the alu s output word size   an arithmetic overflow flag will be set     hardwired into a cpu s circuitry is a set of basic operations it can perform  called an instruction set  such operations may involve  for example  adding or subtracting two numbers  comparing two numbers  or jumping to a different part of a program  each basic operation is represented by a particular combination of bits  known as the machine language opcode  while executing instructions in a machine language program  the cpu decides which operation to perform by  decoding  the opcode  a complete machine language instruction consists of an opcode and  in many cases  additional bits that specify arguments for the operation  for example  the numbers to be summed in the case of an addition operation   going up the complexity scale  a machine language program is a collection of machine language instructions that the cpu executes     the actual mathematical operation for each instruction is performed by a combinational logic circuit within the cpu s processor known as the arithmetic logic unit or alu  in general  a cpu executes an instruction by fetching it from memory  using its alu to perform an operation  and then storing the result to memory  beside the instructions for integer mathematics and logic operations  various other machine instructions exists  such as those for loading data from memory and storing it back  branching operations  and mathematical operations on floating point numbers performed by the cpu s floating point unit  fpu        the control unit of the cpu contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions  the control unit does not execute program instructions  rather  it directs other parts of the system to do so  the control unit communicates with both the alu and memory     the arithmetic logic unit  alu  is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations  the inputs to the alu are the data words to be operated on  called operands   status information from previous operations  and a code from the control unit indicating which operation to perform  depending on the instruction being executed  the operands may come from internal cpu registers or external memory  or they may be constants generated by the alu itself     when all input signals have settled and propagated through the alu circuitry  the result of the performed operation appears at the alu s outputs  the result consists of both a data word  which may be stored in a register or memory  and status information that is typically stored in a special  internal cpu register reserved for this purpose     every cpu represents numerical values in a specific way  for example  some early digital computers represented numbers as familiar decimal  base     numeral system values  and others have employed more unusual representations such as ternary  base three   nearly all modern cpus represent numbers in binary form  with each digit being represented by some two valued physical quantity such as a  high  or  low  voltage f    related to numeric representation is the size and precision of integer numbers that a cpu can represent  in the case of a binary cpu  this is measured by the number of bits  significant digits of a binary encoded integer  that the cpu can process in one operation  which is commonly called  word size    bit width    data path width    integer precision   or  integer size   a cpu s integer size determines the range of integer values it can directly operate on g for example  an   bit cpu can directly manipulate integers represented by eight bits  which have a range of          discrete integer values     integer range can also affect the number of memory locations the cpu can directly address  an address is an integer value representing a specific memory location   for example  if a binary cpu uses    bits to represent a memory address then it can directly address     memory locations  to circumvent this limitation and for various other reasons  some cpus use mechanisms  such as bank switching  that allow additional memory to be addressed     cpus with larger word sizes require more circuitry and consequently are physically larger  cost more  and consume more power  and therefore generate more heat   as a result  smaller    or   bit microcontrollers are commonly used in modern applications even though cpus with much larger word sizes  such as             even     bit  are available  when higher performance is required  however  the benefits of a larger word size  larger data ranges and address spaces  may outweigh the disadvantages     to gain some of the advantages afforded by both lower and higher bit lengths  many cpus are designed with different bit widths for different portions of the device  for example  the ibm system     used a cpu that was primarily    bit  but it used     bit precision inside its floating point units to facilitate greater accuracy and range in floating point numbers   many later cpu designs use similar mixed bit width  especially when the processor is meant for general purpose usage where a reasonable balance of integer and floating point capability is required     most cpus are synchronous circuits  which means they employ a clock signal to pace their sequential operations  the clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave  the frequency of the clock pulses determines the rate at which a cpu executes instructions and  consequently  the faster the clock  the more instructions the cpu will execute each second     to ensure proper operation of the cpu  the clock period is longer than the maximum time needed for all signals to propagate  move  through the cpu  in setting the clock period to a value well above the worst case propagation delay  it is possible to design the entire cpu and the way it moves data around the  edges  of the rising and falling clock signal  this has the advantage of simplifying the cpu significantly  both from a design perspective and a component count perspective  however  it also carries the disadvantage that the entire cpu must wait on its slowest elements  even though some portions of it are much faster  this limitation has largely been compensated for by various methods of increasing cpu parallelism  see below      however  architectural improvements alone do not solve all of the drawbacks of globally synchronous cpus  for example  a clock signal is subject to the delays of any other electrical signal  higher clock rates in increasingly complex cpus make it more difficult to keep the clock signal in phase  synchronized  throughout the entire unit  this has led many modern cpus to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the cpu to malfunction  another major issue  as clock rates increase dramatically  is the amount of heat that is dissipated by the cpu  the constantly changing clock causes many components to switch regardless of whether they are being used at that time  in general  a component that is switching uses more energy than an element in a static state  therefore  as clock rate increases  so does energy consumption  causing the cpu to require more heat dissipation in the form of cpu cooling solutions     one method of dealing with the switching of unneeded components is called clock gating  which involves turning off the clock signal to unneeded components  effectively disabling them   however  this is often regarded as difficult to implement and therefore does not see common usage outside of very low power designs  one notable recent cpu design that uses extensive clock gating is the ibm powerpc based xenon used in the xbox      that way  power requirements of the xbox     are greatly reduced    another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether  while removing the global clock signal makes the design process considerably more complex in many ways  asynchronous  or clockless  designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs  while somewhat uncommon  entire asynchronous cpus have been built without utilizing a global clock signal  two notable examples of this are the arm compliant amulet and the mips r     compatible minimips     rather than totally removing the clock signal  some cpu designs allow certain portions of the device to be asynchronous  such as using asynchronous alus in conjunction with superscalar pipelining to achieve some arithmetic performance gains  while it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts  it is evident that they do at least excel in simpler math operations  this  combined with their excellent power consumption and heat dissipation properties  makes them very suitable for embedded computers       the description of the basic operation of a cpu offered in the previous section describes the simplest form that a cpu can take  this type of cpu  usually referred to as subscalar  operates on and executes one instruction on one or two pieces of data at a time     this process gives rise to an inherent inefficiency in subscalar cpus  since only one instruction is executed at a time  the entire cpu must wait for that instruction to complete before proceeding to the next instruction  as a result  the subscalar cpu gets  hung up  on instructions which take more than one clock cycle to complete execution  even adding a second execution unit  see below  does not improve performance much  rather than one pathway being hung up  now two pathways are hung up and the number of unused transistors is increased  this design  wherein the cpu s execution resources can operate on only one instruction at a time  can only possibly reach scalar performance  one instruction per clock   however  the performance is nearly always subscalar  less than one instruction per cycle      attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the cpu to behave less linearly and more in parallel  when referring to parallelism in cpus  two terms are generally used to classify these design techniques  instruction level parallelism  ilp  seeks to increase the rate at which instructions are executed within a cpu  that is  to increase the utilization of on die execution resources   and thread level parallelism  tlp  purposes to increase the number of threads  effectively individual programs  that a cpu can execute simultaneously  each methodology differs both in the ways in which they are implemented  as well as the relative effectiveness they afford in increasing the cpu s performance for an application h    one of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing  this is the simplest form of a technique known as instruction pipelining  and is utilized in almost all modern general purpose cpus  pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages  this separation can be compared to an assembly line  in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired     pipelining does  however  introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation  a condition often termed data dependency conflict  to cope with this  additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs  naturally  accomplishing this requires additional circuitry  so pipelined processors are more complex than subscalar ones  though not very significantly so   a pipelined processor can become very nearly scalar  inhibited only by pipeline stalls  an instruction spending more than one clock cycle in a stage      further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of cpu components even further  designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units    in a superscalar pipeline  multiple instructions are read and passed to a dispatcher  which decides whether or not the instructions can be executed in parallel  simultaneously   if so they are dispatched to available execution units  resulting in the ability for several instructions to be executed simultaneously  in general  the more instructions a superscalar cpu is able to dispatch simultaneously to waiting execution units  the more instructions will be completed in a given cycle     most of the difficulty in the design of a superscalar cpu architecture lies in creating an effective dispatcher  the dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel  as well as dispatch them in such a way as to keep as many execution units busy as possible  this requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of cpu cache  it also makes hazard avoiding techniques like branch prediction  speculative execution  and out of order execution crucial to maintaining high levels of performance  by attempting to predict which branch  or path  a conditional instruction will take  the cpu can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed  speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes  out of order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies  also in case of single instructions multiple dataxa   a case when a lot of data from the same type has to be processed  modern processors can disable parts of the pipeline so that when a single instruction is executed many times  the cpu skips the fetch and decode phases and thus greatly increases performance on certain occasions  especially in highly monotonous program engines such as video creation software and photo processing     in the case where a portion of the cpu is superscalar and part is not  the part which is not suffers a performance penalty due to scheduling stalls  the intel p  pentium had two superscalar alus which could accept one instruction per clock each  but its fpu could not accept one instruction per clock  thus the p  was integer superscalar but not floating point superscalar  intel s successor to the p  architecture  p   added superscalar capabilities to its floating point features  and therefore afforded a significant increase in floating point instruction performance     both simple pipelining and superscalar design increase a cpu s ilp by allowing a single processor to complete execution of instructions at rates surpassing one instruction per cycle  ipc  i most modern cpu designs are at least somewhat superscalar  and nearly all general purpose cpus designed in the last decade are superscalar  in later years some of the emphasis in designing high ilp computers has been moved out of the cpu s hardware and into its software interface  or isa  the strategy of the very long instruction word  vliw  causes some ilp to become implied directly by the software  reducing the amount of work the cpu must perform to boost ilp and thereby reducing the design s complexity     another strategy of achieving performance is to execute multiple programs or threads in parallel  this area of research is known as parallel computing  in flynn s taxonomy  this strategy is known as multiple instructions multiple data or mimd     one technology used for this purpose was multiprocessing  mp   the initial flavor of this technology is known as symmetric multiprocessing  smp   where a small number of cpus share a coherent view of their memory system  in this scheme  each cpu has additional hardware to maintain a constantly up to date view of memory  by avoiding stale views of memory  the cpus can cooperate on the same program and programs can migrate from one cpu to another  to increase the number of cooperating cpus beyond a handful  schemes such as non uniform memory access  numa  and directory based coherence protocols were introduced in the     s  smp systems are limited to a small number of cpus while numa systems have been built with thousands of processors  initially  multiprocessing was built using multiple discrete cpus and boards to implement the interconnect between the processors  when the processors and their interconnect are all implemented on a single silicon chip  the technology is known as a multi core processor     it was later recognized that finer grain parallelism existed with a single program  a single program might have several threads  or functions  that could be executed separately or in parallel  some of the earliest examples of this technology implemented input output processing such as direct memory access as a separate thread from the computation thread  a more general approach to this technology was introduced in the     s when systems were designed to run multiple computation threads in parallel  this technology is known as multi threading  mt   this approach is considered more cost effective than multiprocessing  as only a small number of components within a cpu is replicated to support mt as opposed to the entire cpu in the case of mp  in mt  the execution units and the memory system including the caches are shared among multiple threads  the downside of mt is that the hardware support for multithreading is more visible to software than that of mp and thus supervisor software like operating systems have to undergo larger changes to support mt  one type of mt that was implemented is known as block multithreading  where one thread is executed until it is stalled waiting for data to return from external memory  in this scheme  the cpu would then quickly switch to another thread which is ready to run  the switch often done in one cpu clock cycle  such as the ultrasparc technology  another type of mt is known as simultaneous multithreading  where instructions of multiple threads are executed in parallel within one cpu clock cycle     for several decades from the     s to early     s  the focus in designing high performance general purpose cpus was largely on achieving high ilp through technologies such as pipelining  caches  superscalar execution  out of order execution  etc  this trend culminated in large  power hungry cpus such as the intel pentium    by the early     s  cpu designers were thwarted from achieving higher performance from ilp techniques due to the growing disparity between cpu operating frequencies and main memory operating frequencies as well as escalating cpu power dissipation owing to more esoteric ilp techniques     cpu designers then borrowed ideas from commercial computing markets such as transaction processing  where the aggregate performance of multiple programs  also known as throughput computing  was more important than the performance of a single thread or program     this reversal of emphasis is evidenced by the proliferation of dual and multiple core cmp  chip level multiprocessing  designs and notably  intel s newer designs resembling its less superscalar p  architecture  late designs in several processor families exhibit cmp  including the x      opteron and athlon    x   the sparc ultrasparc t   ibm power  and power   as well as several video game console cpus like the xbox     s triple core powerpc design  and the ps  s   core cell microprocessor     a less common but increasingly important paradigm of cpus  and indeed  computing in general  deals with data parallelism  the processors discussed earlier are all referred to as some type of scalar device j as the name implies  vector processors deal with multiple pieces of data in the context of one instruction  this contrasts with scalar processors  which deal with one piece of data for every instruction  using flynn s taxonomy  these two schemes of dealing with data are generally referred to as simd  single instruction  multiple data  and sisd  single instruction  single data   respectively  the great utility in creating cpus that deal with vectors of data lies in optimizing tasks that tend to require the same operation  for example  a sum or a dot product  to be performed on a large set of data  some classic examples of these types of tasks are multimedia applications  images  video  and sound   as well as many types of scientific and engineering tasks  whereas a scalar cpu must complete the entire process of fetching  decoding  and executing each instruction and value in a set of data  a vector cpu can perform a single operation on a comparatively large set of data with one instruction  of course  this is only possible when the application tends to require many steps which apply one operation to a large set of data     most early vector cpus  such as the cray    were associated almost exclusively with scientific research and cryptography applications  however  as multimedia has largely shifted to digital media  the need for some form of simd in general purpose cpus has become significant  shortly after inclusion of floating point execution units started to become commonplace in general purpose processors  specifications for and implementations of simd execution units also began to appear for general purpose cpus  some of these early simd specifications like hp s multimedia acceleration extensions  max  and intel s mmx were integer only  this proved to be a significant impediment for some software developers  since many of the applications that benefit from simd primarily deal with floating point numbers  progressively  these early designs were refined and remade into some of the common  modern simd specifications  which are usually associated with one isa  some notable modern examples are intel s sse and the powerpc related altivec  also known as vmx  k    the performance or speed of a processor depends on  among many other factors  the clock rate  generally given in multiples of hertz  and the instructions per clock  ipc   which together are the factors for the instructions per second  ips  that the cpu can perform    many reported ips values have represented  peak  execution rates on artificial instruction sequences with few branches  whereas realistic workloads consist of a mix of instructions and applications  some of which take longer to execute than others  the performance of the memory hierarchy also greatly affects processor performance  an issue barely considered in mips calculations  because of these problems  various standardized tests  often called  benchmarks  for this purpose such as specint   have been developed to attempt to measure the real effective performance in commonly used applications     processing performance of computers is increased by using multi core processors  which essentially is plugging two or more individual processors  called cores in this sense  into one integrated circuit    ideally  a dual core processor would be nearly twice as powerful as a single core processor  in practice  the performance gain is far smaller  only about      due to imperfect software algorithms and implementation    increasing the number of cores in a processor  i e  dual core  quad core  etc   increases the workload that can be handled  this means that the processor can now handle numerous asynchronous events  interrupts  etc  which can take a toll on the cpu when overwhelmed  these cores can be thought of as different floors in a processing plant  with each floor handling a different task  sometimes  these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information     due to specific capabilities of modern cpus  such as hyper threading and uncore  which involve sharing of actual cpu resources while aiming at increased utilization  monitoring performance levels and hardware utilization gradually became a more complex task  as a response  some cpus implement additional hardware logic that monitors actual utilization of various parts of a cpu and provides various counters accessible to software  an example is intel s performance counter monitor technology   