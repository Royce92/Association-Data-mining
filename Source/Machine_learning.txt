 machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data   such algorithms operate by building a model from example inputs and using that to make predictions or decisions     rather than following strictly static program instructions  machine learning is closely related to and often overlaps with computational statistics  a discipline which also specializes in prediction making     machine learning is a subfield of computer science stemming from research into artificial intelligence   it has strong ties to statistics and mathematical optimization  which deliver methods  theory and application domains to the field  machine learning is employed in a range of computing tasks where designing and programming explicit  rule based algorithms is infeasible  example applications include spam filtering  optical character recognition  ocr    search engines and computer vision  machine learning is sometimes conflated with data mining   although that focuses more on exploratory data analysis   machine learning and pattern recognition  can be viewed as two facets of the same field    vii    when employed in industrial contexts  machine learning methods may be referred to as predictive analytics or predictive modelling             in       arthur samuel defined machine learning as a  field of study that gives computers the ability to learn without being explicitly programmed       tom m  mitchell provided a widely quoted  more formal definition   a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p  if its performance at tasks in t  as measured by p  improves with experience e    this definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms  thus following alan turing s proposal in turing s paper  computing machinery and intelligence  that the question  can machines think   be replaced with the question  can machines do what we  as thinking entities  can do        machine learning tasks are typically classified into three broad categories  depending on the nature of the learning  signal  or  feedback  available to a learning system  these are       between supervised and unsupervised learning is semi supervised learning  where the teacher gives an incomplete training signal  a training set with some  often many  of the target outputs missing  transduction is a special case of this principle where the entire set of problem instances is known at learning time  except that part of the targets are missing     among other categories of machine learning problems  learning to learn learns its own inductive bias based on previous experience  developmental learning  elaborated for robot learning  generates its own sequences  also called curriculum  of learning situations to cumulatively acquire repertoires of novel skills through autonomous self exploration and social interaction with human teachers  and using guidance mechanisms such as active learning  maturation  motor synergies  and imitation     another categorization of machine learning tasks arises when one considers the desired output of a machine learned system        as a scientific endeavour  machine learning grew out of the quest for artificial intelligence  already in the early days of ai as an academic discipline  some researchers were interested in having machines learn from data  they attempted to approach the problem with various symbolic methods  as well as what were then termed  neural networks   mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics  probabilistic reasoning was also employed  especially in automated medical diagnosis           however  an increasing emphasis on the logical  knowledge based approach caused a rift between ai and machine learning  probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation        by       expert systems had come to dominate ai  and statistics was out of favor    work on symbolic knowledge based learning did continue within ai  leading to inductive logic programming  but the more statistical line of research was now outside the field of ai proper  in pattern recognition and information retrieval                 neural networks research had been abandoned by ai and computer science around the same time  this line  too  was continued outside the ai cs field  as  connectionism   by researchers from other disciplines including hopfield  rumelhart and hinton  their main success came in the mid     s with the reinvention of backpropagation          machine learning  reorganized as a separate field  started to flourish in the     s  the field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature  it shifted focus away from the symbolic approaches it had inherited from ai  and toward methods and models borrowed from statistics and probability theory    it also benefited from the increasing availability of digitized information  and the possibility to distribute that via the internet     machine learning and data mining often employ the same methods and overlap significantly  they can be roughly distinguished as follows     the two areas overlap in many ways  data mining uses many machine learning methods  but often with a slightly different goal in mind  on the other hand  machine learning also employs data mining methods as  unsupervised learning  or as a preprocessing step to improve learner accuracy  much of the confusion between these two research communities  which do often have separate conferences and separate journals  ecml pkdd being a major exception  comes from the basic assumptions they work with  in machine learning  performance is usually evaluated with respect to the ability to reproduce known knowledge  while in knowledge discovery and data mining  kdd  the key task is the discovery of previously unknown knowledge  evaluated with respect to known knowledge  an uninformed  unsupervised  method will easily be outperformed by supervised methods  while in a typical kdd task  supervised methods cannot be used due to the unavailability of training data     machine learning also has intimate ties to optimization  many learning problems are formulated as minimization of some loss function on a training set of examples  loss functions expresses the discrepancy between the predictions of the model being trained and the actual problem instances  for example  in classification  one wants to assign a label to instances  and models are trained to correctly predict the pre assigned labels of a set examples   the difference between the two fields arises from the goal of generalization  while optimization algorithms can minimize the loss on a training set  machine learning is concerned with minimizing the loss on unseen samples       machine learning and statistics are closely related fields  according to michael i  jordan  the ideas of machine learning  from methodological principles to theoretical tools  have had a long pre history in statistics    he also suggested the term data science as a placeholder to call the overall field       leo breiman distinguished two statistical modelling paradigms  data model and algorithmic model    wherein  algorithmic model  means more or less the machine learning algorithms like random forest     some statisticians have adopted methods from machine learning  leading to a combined field that they call statistical learning       a core objective of a learner is to generalize from its experience  full citation needed   generalization in this context is the ability of a learning machine to perform accurately on new  unseen examples tasks after having experienced a learning data set  the training examples come from some generally unknown probability distribution  considered representative of the space of occurrences  and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases     the computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory  because training sets are finite and the future is uncertain  learning theory usually does not yield guarantees of the performance of algorithms  instead  probabilistic bounds on the performance are quite common  the bias variance decomposition is one way to quantify generalization error     in addition to performance bounds  computational learning theorists study the time complexity and feasibility of learning  in computational learning theory  a computation is considered feasible if it can be done in polynomial time  there are two kinds of time complexity results  positive results show that a certain class of functions can be learned in polynomial time  negative results show that certain classes cannot be learned in polynomial time     there are many similarities between machine learning theory and statistical inference  although they use different terms     decision tree learning uses a decision tree as a predictive model  which maps observations about an item to conclusions about the item s target value     association rule learning is a method for discovering interesting relations between variables in large databases     an artificial neural network  ann  learning algorithm  usually called  neural network   nn   is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks  computations are structured in terms of an interconnected group of artificial neurons  processing information using a connectionist approach to computation  modern neural networks are non linear statistical data modeling tools  they are usually used to model complex relationships between inputs and outputs  to find patterns in data  or to capture the statistical structure in an unknown joint probability distribution between observed variables     inductive logic programming  ilp  is an approach to rule learning using logic programming as a uniform representation for input examples  background knowledge  and hypotheses  given an encoding of the known background knowledge and a set of examples represented as a logical database of facts  an ilp system will derive a hypothesized logic program that entails all positive and no negative examples  inductive programming is a related field that considers any kind of programming languages for representing hypotheses  and not only logic programming   such as functional programs     support vector machines  svms  are a set of related supervised learning methods used for classification and regression  given a set of training examples  each marked as belonging to one of two categories  an svm training algorithm builds a model that predicts whether a new example falls into one category or the other     cluster analysis is the assignment of a set of observations into subsets  called clusters  so that observations within the same cluster are similar according to some predesignated criterion or criteria  while observations drawn from different clusters are dissimilar  different clustering techniques make different assumptions on the structure of the data  often defined by some similarity metric and evaluated for example by internal compactness  similarity between members of the same cluster  and separation between different clusters  other methods are based on estimated density and graph connectivity  clustering is a method of unsupervised learning  and a common technique for statistical data analysis     a bayesian network  belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph  dag   for example  a bayesian network could represent the probabilistic relationships between diseases and symptoms  given symptoms  the network can be used to compute the probabilities of the presence of various diseases  efficient algorithms exist that perform inference and learning     reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long term reward  reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states  reinforcement learning differs from the supervised learning problem in that correct input output pairs are never presented  nor sub optimal actions explicitly corrected     several learning algorithms  mostly unsupervised learning algorithms  aim at discovering better representations of the inputs provided during training  classical examples include principal components analysis and cluster analysis  representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful  often as a pre processing step before performing classification or predictions  allowing to reconstruct the inputs coming from the unknown data generating distribution  while not being necessarily faithful for configurations that are implausible under that distribution     manifold learning algorithms attempt to do so under the constraint that the learned representation is low dimensional  sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse  has many zeros   multilinear subspace learning algorithms aim to learn low dimensional representations directly from tensor representations for multidimensional data  without reshaping them into  high dimensional  vectors    deep learning algorithms discover multiple levels of representation  or a hierarchy of features  with higher level  more abstract features defined in terms of  or generating  lower level features  it has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data       in this problem  the learning machine is given pairs of examples that are considered similar and pairs of less similar objects  it then needs to learn a similarity function  or a distance metric function  that can predict if new objects are similar  it is sometimes used in recommendation systems     in this method  a datum is represented as a linear combination of basis functions  and the coefficients are assumed to be sparse  let x be a d dimensional datum  d be a d by n matrix  where each column of d represents a basis function  r is the coefficient to represent x using d  mathematically  sparse dictionary learning means the following  where r is sparse  generally speaking  n is assumed to be larger than d to allow the freedom for a sparse representation     learning a dictionary along with sparse representations is strongly np hard and also difficult to solve approximately    a popular heuristic method for sparse dictionary learning is k svd     sparse dictionary learning has been applied in several contexts  in classification  the problem is to determine which classes a previously unseen datum belongs to  suppose a dictionary for each class has already been built  then a new datum is associated with the class such that it s best sparsely represented by the corresponding dictionary  sparse dictionary learning has also been applied in image de noising  the key idea is that a clean image path can be sparsely represented by an image dictionary  but the noise cannot       a genetic algorithm  ga  is a search heuristic that mimics the process of natural selection  and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem  in machine learning  genetic algorithms found some uses in the     s and     s         applications for machine learning include     in       the online movie company netflix held the first  netflix prize  competition to find a program to better predict user preferences and improve the accuracy on its existing cinematch movie recommendation algorithm by at least      a joint team made up of researchers from at t labs research in collaboration with the teams big chaos and pragmatic theory built an ensemble model to win the grand prize in      for    million    shortly after the prize was awarded  netflix realized that viewers  ratings were not the best indicators of their viewing patterns   everything is a recommendation   and they changed their recommendation engine accordingly       in      the wall street journal wrote about a money management firm rebellion research s use of machine learning to predict economic movements  the article talks about rebellion research s prediction of the financial crisis and economic recovery       in      it has been reported that a machine learning algorithm has been applied in art history to study fine art paintings  and that it may have revealed previously unrecognized influences between artists       software suites containing a variety of machine learning algorithms include the following  