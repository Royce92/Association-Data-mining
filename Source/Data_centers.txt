 a data center is a facility used to house computer systems and associated components  such as telecommunications and storage systems  it generally includes redundant or backup power supplies  redundant data communications connections  environmental controls  e g   air conditioning  fire suppression  and various security devices  large data centers are industrial scale operations using as much electricity as a small town               data centers have their roots in the huge computer rooms of the early ages of the computing industry  early computer systems were complex to operate and maintain  and required a special environment in which to operate  many cables were necessary to connect all the components  and methods to accommodate and organize these were devised  such as standard racks to mount equipment  raised floors  and cable trays  installed overhead or under the elevated floor   also  a single mainframe required a great deal of power  and had to be cooled to avoid overheating  security was importantxa   computers were expensive  and were often used for military purposes  basic design guidelines for controlling access to the computer room were therefore devised     during the boom of the microcomputer industry  and especially during the     s  computers started to be deployed everywhere  in many cases with little or no care about operating requirements  however  as information technology  it  operations started to grow in complexity  companies grew aware of the need to control it resources  with the advent of unix and the subsequent proliferation of freely available linux compatible pc operating systems during the     s  these were called  servers  as timesharing operating systems like unix rely heavily on the client server model to facilitate sharing unique resources between multiple users  the availability of inexpensive networking equipment  coupled with new standards for network structured cabling  made it possible to use a hierarchical design that put the servers in a specific room inside the company  the use of the term  data center   as applied to specially designed computer rooms  started to gain popular recognition about this time     the boom of data centers came during the dot com bubble  companies needed fast internet connectivity and nonstop operation to deploy systems and establish a presence on the internet  installing such equipment was not viable for many smaller companies  many companies started building very large facilities  called internet data centers  idcs   which provide businesses with a range of solutions for systems deployment and operation  new technologies and practices were designed to handle the scale and the operational requirements of such large scale operations  these practices eventually migrated toward the private data centers  and were adopted largely because of their practical results  data centers for cloud computing are called cloud data centers  cdcs   but nowadays  the division of these terms has almost disappeared and they are being integrated into a term  data center      with an increase in the uptake of cloud computing  business and government organizations are scrutinizing data centers to a higher degree in areas such as security  availability  environmental impact and adherence to standards  standard documents from accredited professional groups  such as the telecommunications industry association  specify the requirements for data center design  well known operational metrics for data center availability can be used to evaluate the business impact of a disruption  there is still a lot of development being done in operation practice  and also in environmentally friendly data center design  data centers are typically very expensive to build and maintain     it operations are a crucial aspect of most organizational operations around the world  one of the main concerns is business continuity  companies rely on their information systems to run their operations  if a system becomes unavailable  company operations may be impaired or stopped completely  it is necessary to provide a reliable infrastructure for it operations  in order to minimize any chance of disruption  information security is also a concern  and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach  a data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment  this is accomplished through redundancy of mechanical cooling and power systems  including emergency backup power generators serving the data center along with fiber optic cables     the telecommunications industry association s tia     telecommunications infrastructure standard for data centers  specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi tenant internet hosting data centers  the topology proposed in this document is intended to be applicable to any size data center      telcordia gr       nebs requirements for telecommunications data center equipment and spaces  provides guidelines for data center spaces within telecommunications networks  and environmental requirements for the equipment intended for installation in those spaces  these criteria were developed jointly by telcordia and industry representatives  they may be applied to data center spaces housing data processing or information technology  it  equipment  the equipment may be used to     effective data center operation requires a balanced investment in both the facility and the housed equipment  the first step is to establish a baseline facility environment suitable for equipment installation  standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers     standardization means integrated building and equipment engineering  modularity has the benefits of scalability and easier growth  even when planning forecasts are less than optimal  for these reasons  telecommunications data centers should be planned in repetitive building blocks of equipment  and associated power and support  conditioning  equipment when practical  the use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction  or perhaps worsexa   under construction that fails to meet future needs     the  lights out  data center  also known as a darkened or a dark data center  is a data center that  ideally  has all but eliminated the need for direct access by personnel  except under extraordinary circumstances  because of the lack of need for staff to enter the data center  it can be operated without lighting  all of the devices are accessed and managed by remote systems  with automation programs used to perform unattended operations  in addition to the energy savings  reduction in staffing costs and the ability to locate the site further from population centers  implementing a lights out data center reduces the threat of malicious attacks upon the infrastructure       there is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer it equipment and capabilities  such as cloud computing  this process is also known as data center transformation      organizations are experiencing rapid it growth but their data centers are aging  industry research company international data corporation  idc  puts the average age of a data center at nine years old   gartner  another research company says data centers older than seven years are obsolete      in may       data center research organization uptime institute  reported that    percent of the large companies it surveyed expect to exhaust it capacity within the next    months      data center transformation takes a step by step approach through integrated projects carried out over time  this differs from a traditional method of data center upgrades that takes a serial and siloed approach   the typical projects within a data center transformation initiative include standardization consolidation  virtualization  automation and security     today many data centers are run by internet service providers solely for the purpose of hosting their own and third party servers     however traditionally data centers were either built for the sole use of one large company  or as carrier hotels or network neutral data centers     these facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content servers     the telecommunications industry association is a trade association accredited by ansi  american national standards institute   in      it published ansi tia      telecommunications infrastructure standard for data centers  which defined four levels  called tiers  of data centers in a thorough  quantifiable manner  tia     was amended in      and again in       tia     data center standards overview describes the requirements for the data center infrastructure  the simplest is a tier   data center  which is basically a server room  following basic guidelines for the installation of computer systems  the most stringent level is a tier   data center  which is designed to host mission critical computer systems  with fully redundant subsystems and compartmentalized security zones controlled by biometric access controls methods  another consideration is the placement of the data center in a subterranean context  for data security as well as environmental considerations such as cooling requirements       the german datacenter star audit program uses an auditing process to certify   levels of  gratification  that affect data center criticality     independent from the ansi tia     standard  the uptime institute  a think tank and professional services organization based in santa fe  new mexico  has defined its own four levels  the levels describe the availability of data from the hardware at a location  the higher the tier  the greater the availability  the levels are           the difference between                            and          while seemingly nominal  could be significant depending on the application     whilst no down time is ideal  the tier system allows for unavailability of services as listed below over a period of one year          minutes      the uptime institute also classifies the tiers in different categories  design documents  constructed facility  operational sustainability      a data center can occupy one room of a building  one or more floors  or an entire building  most of the equipment is often in the form of servers mounted in    inch rack cabinets  which are usually placed in single rows forming corridors  so called aisles  between them  this allows people access to the front and rear of each cabinet  servers differ greatly in size from  u servers to large freestanding storage silos which occupy many square feet of floor space  some equipment such as mainframe computers and storage devices are often as big as the racks themselves  and are placed alongside them  very large data centers may use shipping containers packed with       or more servers each    when repairs or upgrades are needed  whole containers are replaced  rather than repairing individual servers        local building codes may govern the minimum ceiling heights     design programming  also known as architectural programming  is the process of researching and making decisions to identify the scope of a design project    other than the architecture of the building itself there are three elements to design programming for data centers  facility topology design  space planning   engineering infrastructure design  mechanical systems such as cooling and electrical systems including power  and technology infrastructure design  cable plant   each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner s performance wishes of the facility over time     various vendors who provide data center design services define the steps of data center design slightly differently  but all address the same basic aspects as given below     modeling criteria are used to develop future state scenarios for space  power  cooling  and costs in the data center    the aim is to create a master plan with parameters such as number  size  location  topology  it floor system layouts  and power and cooling technology and configurations  the purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply     design recommendations plans generally follow the modelling criteria phase  the optimal technology infrastructure is identified and planning criteria are developed  such as critical power capacities  overall data center power requirements using an agreed upon pue  power utilization efficiency   mechanical cooling capacities  kilowatts per cabinet  raised floor space  and the resiliency level for the facility     conceptual designs embody the design recommendations or plans and should take into account  what if  scenarios to ensure all operational outcomes are met in order to future proof the facility  conceptual floor layouts should be driven by it performance requirements as well as lifecycle costs associated with it demand  energy efficiency  cost efficiency and availability  future proofing will also include expansion capabilities  often provided in modern data centers through modular designs  these allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility     detailed design is undertaken once the appropriate conceptual design is determined  typically including a proof of concept  the detailed design phase should include the detailed architectural  structural  mechanical and electrical information and specification of the facility  at this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure  detailed it infrastructure design and it infrastructure documentation are produced     mechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center  such as heating  ventilation and air conditioning  hvac   humidification and dehumidification equipment  pressurization  and so on    this stage of the design process should be aimed at saving space and costs  while ensuring business and reliability objectives are met as well as achieving pue and green requirements    modern designs include modularizing and scaling it loads  and making sure capital spending on the building construction is optimized     electrical engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes  aspects may include utility service planning  distribution  switching and bypass from power sources  uninterruptable power source  ups  systems  and more       these designs should dovetail to energy standards and best practices while also meeting business objectives  electrical configurations should be optimized and operationally compatible with the data center user s capabilities  modern electrical design is modular and scalable    and is available for low and medium voltage requirements as well as dc  direct current      technology infrastructure design addresses the telecommunications cabling systems that run throughout data centers  there are cabling systems for all data center environments  including horizontal cabling  voice  modem  and facsimile telecommunications services  premises switching equipment  computer and telecommunications management connections  keyboard video mouse connections and data communications    wide area  local area  and storage area networks should link with other building signaling systems  e g  fire  security  power  hvac  ems      the higher the availability needs of a data center  the higher the capital and operational costs of building and managing it  business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of it systems estimated cost analyses from modeled scenarios  in other words  how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime  if the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses  a higher level of availability should be factored into the data center design  if the cost of avoiding downtime greatly exceeds the cost of downtime itself  a lower level of availability should be factored into the design       aspects such as proximity to available power grids  telecommunications infrastructure  networking services  transportation lines and emergency services can affect costs  risk  security and other factors to be taken into consideration for data center design  whilst a wide array of location factors are taken into account  e g  flight paths  neighbouring uses  geological risks  access to suitable available power is often the longest lead time item  location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed  in turn this impacts uptime and the costs associated with cooling    for example  the topology and the cost of managing a data center in a warm  humid climate will vary greatly from managing one in a cool  dry climate     modularity and flexibility are key elements in allowing for a data center to grow and change over time  data center modules are pre engineered  standardized building blocks that can be easily configured and moved as needed       a modular data center may consist of data center equipment contained within shipping containers or similar portable containers    but it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed  moved or added to quickly as needs change       the physical environment of a data center is rigorously controlled  air conditioning is used to control the temperature and humidity in the data center  ashrae s  thermal guidelines for data processing environments    recommends a temperature range of      xa  c       xa  f   a dew point range of     xa  c       xa  f   and a maximum relative humidity of     for data center environments    the temperature in a data center will naturally rise because the electrical power used heats the air  unless the heat is removed  the ambient temperature will rise  resulting in electronic equipment malfunction  by controlling the air temperature  the server components at the board level are kept within the manufacturer s specified temperature humidity range  air conditioning systems help control humidity by cooling the return space air below the dew point  too much humidity  and water may begin to condense on internal components  in case of a dry atmosphere  ancillary humidification systems may add water vapor if the humidity is too low  which can result in static electricity discharge problems which may damage components  subterranean data centers may keep computer equipment cool while expending less energy than conventional designs     modern data centers try to use economizer cooling  where they use outside air to keep the data center cool  at least one data center  located in upstate new york  will cool servers using outside air during the winter  they do not use chillers air conditioners  which creates potential energy savings in the millions    increasingly indirect air cooling is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center     telcordia gr       nebs  raised floor generic requirements for network and data centers  presents generic engineering requirements for raised floors that fall within the strict nebs guidelines     there are many types of commercially available floors that offer a wide range of structural strength and loading capabilities  depending on component construction and the materials used  the general types of raised floors include stringerless  stringered  and structural platforms  all of which are discussed in detail in gr      and summarized below     data centers typically have raised flooring made up of   xa cm   xa ft  removable square tiles  the trend is towards       xa cm       xa in  void to cater for better and uniform air distribution  these provide a plenum for air to circulate below the floor  as part of the air conditioning system  as well as providing space for power cabling     raised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with zinc whiskers in the past  and likely are still present in many data centers  this happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion  maintenance on a raised floor or installing of cable etc  can dislodge the whiskers  which enter the airflow and may short circuit server components or power supplies  sometimes through a high current metal vapor plasma arc  this phenomenon is not unique to data centers  and has also caused catastrophic failures of satellites and military hardware       backup power consists of one or more uninterruptible power supplies  battery banks  and or diesel   gas turbine generators       to prevent single points of failure  all elements of the electrical systems  including backup systems  are typically fully duplicated  and critical servers are connected to both the  a side  and  b side  power feeds  this arrangement is often made to achieve n   redundancy in the systems  static transfer switches are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure     data cabling is typically routed through overhead cable trays in modern data centers  but somewho  are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary  smaller less expensive data centers without raised flooring may use anti static tiles for a flooring surface  computer cabinets are often organized into a hot aisle arrangement to maximize airflow efficiency     data centers feature fire protection systems  including passive and active design elements  as well as implementation of fire prevention programs in operations  smoke detectors are usually installed to provide early warning of a fire at its incipient stage  this allows investigation  interruption of power  and manual fire suppression using hand held fire extinguishers before the fire grows to a large size  an active fire protection system  such as a fire sprinkler system or a clean agent fire suppression gaseous system  is often provided to control a full scale fire if it develops  high sensitivity smoke detectors  such as aspirating smoke detectors  activating clean agent fire suppression gaseous systems activate earlier than fire sprinklers  sprinklers   structure protection   building life safety  clean agents   business continuity   asset protection  no water   no collateral damage or clean up  passive fire protection elements include the installation of fire walls around the data center  so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems  fire wall penetrations into the server room  such as cable penetrations  coolant line penetrations and air ducts  must be provided with fire rated penetration assemblies  such as fire stopping     physical security also plays a large role with data centers  physical access to the site is usually restricted to selected personnel  with controls including a layered security system often starting with fencing  bollards and mantraps    video camera surveillance and permanent security guards are almost always present if the data center is large or contains sensitive information on any of the systems within  the use of finger print recognition mantraps is starting to be commonplace     energy use is a central issue for data centers  power draw for data centers ranges from a few kw for a rack of servers in a closet to several tens of mw for large facilities  some facilities have power densities more than     times that of a typical office building    for higher power density facilities  electricity costs are a dominant operating expense and account for over     of the total cost of ownership  tco  of a data center    by      the cost of power for the data center is expected to exceed the cost of the original capital investment       in      the entire information and communication technologies or ict sector was estimated to be responsible for roughly    of global carbon emissions with data centers accounting for     of the ict footprint    the us epa estimates that servers and data centers are responsible for up to      of the total us electricity consumption    or roughly     of us ghg emissions    for       given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from      levels by            siting is one of the factors that affect the energy consumption and environmental effects of a datacenter  in areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate  thus countries with favorable conditions  such as  canada    finland    sweden   and switzerland    are trying to attract cloud computing data centers     in an    month investigation by scholars at rice university s baker institute for public policy in houston and the institute for sustainable and applied infodynamics in singapore  data center related emissions will more than triple by             the most commonly used metric to determine the energy efficiency of a data center is power usage effectiveness  or pue  this simple ratio is the total power entering the data center divided by the power used by the it equipment     total facility power consists of power used by it equipment plus any overhead power consumed by anything that is not considered a computing or data communication device  i e  cooling  lighting  etc    an ideal pue is     for the hypothetical situation of zero overhead power  the average data center in the us has a pue of        meaning that the facility uses two watts of total power  overhead   it equipment  for every watt delivered to it equipment  state of the art data center energy efficiency is estimated to be roughly        some large data center operators like microsoft and yahoo  have published projections of pue for facilities in development  google publishes quarterly actual efficiency performance from data centers in operation       the u s  environmental protection agency has an energy star rating for standalone or large data centers  to qualify for the ecolabel  a data center must be within the top quartile of energy efficiency of all reported facilities       european union also has a similar initiative  eu code of conduct for data centres      often  the first step toward curbing energy use in a data center is to understand how energy is being used in the data center  multiple types of analysis exist to measure data center energy use  aspects measured include not just energy used by it equipment itself  but also by the data center facility equipment  such as chillers and fans       power is the largest recurring cost to the user of a data center    a power and cooling analysis  also referred to as a thermal assessment  measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures    a power and cooling analysis can help to identify hot spots  over cooled areas that can handle greater power use density  the breakpoint of equipment loading  the effectiveness of a raised floor strategy  and optimal equipment positioning  such as ac units  to balance temperatures across the data center  power cooling density is a measure of how much square footage the center can cool at maximum capacity       an energy efficiency analysis measures the energy use of data center it and facilities equipment  a typical energy efficiency analysis measures factors such as a data center s power use effectiveness  pue  against industry standards  identifies mechanical and electrical sources of inefficiency  and identifies air management metrics       this type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center predicting the temperature  airflow  and pressure behavior of a data center to assess performance and energy consumption  using numerical modeling    by predicting the effects of these environmental conditions  cfd analysis in the data center can be used to predict the impact of high density racks mixed with low density racks   and the onward impact on cooling resources  poor infrastructure management practices and ac failure of ac shutdown for scheduled maintenance     thermal zone mapping uses sensors and computer modeling to create a three dimensional image of the hot and cool zones in a data center       this information can help to identify optimal positioning of data center equipment  for example  critical servers might be placed in a cool zone that is serviced by redundant ac units     datacenters use a lot of power  consumed by two main usages  the power required to run the actual equipment and then the power required to cool the equipment  the first category is addressed by designing computers and storage systems that are increasingly power efficient   to bring down cooling costs datacenter designers try to use natural ways to cool the equipment  many datacenters are located near good fiber connectivity  power grid connections and also people concentrations to manage the equipment  but there are also circumstances where the datacenter can be miles away from the users and don t need a lot of local management  examples of this are the  mass  datacenters like google or facebook  these dc s are built around many standardized servers and storage arrays and the actual users of the systems are located all around the world  after the initial build of a datacenter staff numbers required to keep it running are often relatively low  especially datacenters that provide mass storage or computing power which don t need to be near population centers  datacenters in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components       communications in data centers today are most often based on networks running the ip protocol suite  data centers contain a set of routers and switches that transport traffic between the servers and to the outside world  redundancy of the internet connection is often provided by using two or more upstream service providers  see multihoming      some of the servers at the data center are used for running the basic internet and intranet services needed by internal users in the organization  e g   e mail servers  proxy servers  and dns servers     network security elements are also usually deployed  firewalls  vpn gateways  intrusion detection systems  etc  also common are monitoring systems for the network and some of the applications  additional off site monitoring systems are also typical  in case of a failure of communications inside the data center     data center infrastructure management  dcim  is the integration of information technology  it  and facility management disciplines to centralize monitoring  management and intelligent capacity planning of a data center s critical systems  achieved through the implementation of specialized software  hardware and sensors  dcim enables common  real time monitoring and management platform for all interdependent systems across it and facility infrastructures     depending on the type of implementation  dcim products can help data center managers identify and eliminate sources of risk to increase availability of critical it systems  dcim products also can be used to identify interdependencies between facility and it infrastructures to alert the facility manager to gaps in system redundancy  and provide dynamic  holistic benchmarks on power consumption and efficiency to measure the effectiveness of  green it  initiatives     measuring and understanding important data center efficiency metrics  a lot of the discussion in this area has focused on energy issues  but other metrics beyond the pue can give a more detailed picture of the data center operations  server  storage  and staff utilization metrics can contribute to a more complete view of an enterprise data center  in many cases  disc capacity goes unused and in many instances the organizations run their servers at     utilization or less    more effective automation tools can also improve the number of servers or virtual machines that a single admin can handle     dcim providers are increasingly linking with computational fluid dynamics providers to predict complex airflow patterns in the data center  the cfd component is necessary to quantify the impact of planned future changes on cooling resilience  capacity and efficiency       the main purpose of a data center is running the it systems applications that handle the core business and operational data of the organization  such systems may be proprietary and developed internally by the organization  or bought from enterprise software vendors  such common applications are erp and crm systems     a data center may be concerned with just operations architecture or it may provide other services as well     often these applications will be composed of multiple hosts  each running a single component  common components of such applications are databases  file servers  application servers  middleware  and various others     data centers are also used for off site backups  companies may subscribe to backup services provided by a data center  this is often used in conjunction with backup tapes  backups can be taken off servers locally on to tapes  however  tapes stored on site pose a security threat and are also susceptible to fire and flooding  larger companies may also send their backups off site for added security  this can be done by backing up to a data center  encrypted backups can be sent over the internet to another data center where they can be stored securely     for quick deployment or disaster recovery  several large hardware vendors have developed mobile solutions that can be installed and made operational in very short time  companies such as cisco systems    sun microsystems  sun modular datacenter       bull  mobull     ibm  portable modular data center   hp  performance optimized datacenter     huawei  container data center solution     and google  google modular data center  have developed systems that could be used for this purpose         according to synergy research group   the scale of the wholesale colocation market in the united states is very significant relative to the retail market  with q  wholesale revenues reaching almost      million  digital realty trust is the wholesale market leader  followed at a distance by dupont fabros   synergy research also describes the us colocation market as the most mature and well developed in the world   based on revenue and the continued adoption of cloud infrastructure services  