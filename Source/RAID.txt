 raid  originally redundant array of inexpensive disks  now commonly redundant array of independent disks  is a data storage virtualization technology that combines multiple disk drive components into a logical unit for the purposes of data redundancy or performance improvement      data is distributed across the drives in one of several ways  referred to as raid levels  depending on the specific level of redundancy and performance required  the different schemes or architectures are named by the word raid followed by a number  e g  raidxa    raidxa     each scheme provides a different balance between the key goals  reliability  availability  performance  and capacity  raid levels greater than raidxa   provide protection against unrecoverable  sector  read errors  as well as whole disk failure             the term  raid  was invented by david patterson  garth a  gibson  and randy katz at the university of california  berkeley in       in their june      paper  a case for redundant arrays of inexpensive disks  raid    presented at the sigmod conference  they argued that the top performing mainframe disk drives of the time could be beaten on performance by an array of the inexpensive drives that had been developed for the growing personal computer market  although failures would rise in proportion to the number of drives  by configuring for redundancy  the reliability of an array could far exceed that of any large single drive      although not yet using that terminology  the technologies of the five levels of raid named in the paper were used in various products prior to the paper s publication   including the following     industry raid manufacturers later tended to interpret the acronym as standing for  redundant array of independent disks              many raid levels employ an error protection scheme called  parity   a widely used method in information technology to provide fault tolerance in a given set of data  most use simple xor  but raidxa   uses two separate parities based respectively on addition and multiplication in a particular galois field or reed solomon error correction       raid can also provide data security with solid state drives  ssds  without the expense of an all ssd system  for example  a fast ssd can be mirrored with a mechanical drive  for this configuration to provide a significant speed advantage an appropriate controller is needed that uses the fast ssd for all read operations  adaptec calls this  hybrid raid        a number of standard schemes have evolved  these are called levels  originally  there were five raid levels  but many variations have evolved notably several nested levels and many non standard levels  mostly proprietary   raid levels and their associated data formats are standardized by the storage networking industry association  snia  in the common raid disk drive format  ddf  standard         in what was originally termed hybrid raid    many storage controllers allow raid levels to be nested  the elements of a raid may be either individual drives or arrays themselves  arrays are rarely nested more than one level deep       the final array is known as the top array  when the top array is raidxa    such as in raidxa     and raidxa       most vendors omit the      yielding raidxa    and raidxa     respectively      many configurations other than the basic numbered raid levels are possible  and many companies  organizations  and groups have created their own non standard configurations  in many cases designed to meet the specialized needs of a small niche group  such configurations include the following     the distribution of data across multiple drives can be managed either by dedicated computer hardware or by software  a software solution may be part of the operating system  or it may be part of the firmware and drivers supplied with a hardware raid controller     software raid implementations are now provided by many operating systems  software raid can be implemented as     some advanced file systems are designed to organize data across multiple storage devices directly  without needing the help of a third party logical volume manager      many operating systems include basic raid implementations     if a boot drive fails  the system has to be sophisticated enough to be able to boot off the remaining drive or drives  for instance  consider a computer whose disk is configured as raidxa    mirrored drives   if the first drive in the array fails  then a first stage boot loader might not be sophisticated enough to attempt loading the second stage boot loader from the second drive as a fallback  the second stage boot loader for freebsd is capable of loading a kernel from such an array       software implemented raid is not always compatible with the system s boot process  and it is generally impractical for desktop versions of windows  however  hardware raid controllers are expensive and proprietary  to fill this gap  cheap  raid controllers  were introduced that do not contain a dedicated raid controller chip  but simply a standard drive controller chip with proprietary firmware and drivers  during early bootup  the raid is implemented by the firmware and  once the operating system has been more completely loaded  the drivers take over control  consequently  such controllers may not work when driver support is not available for the host operating system    an example is intel matrix raid  implemented on many consumer level motherboards         because some minimal hardware support is involved  this implementation approach is also called  hardware assisted software raid          hybrid model  raid    or even  fake raid     if raidxa   is supported  the hardware may provide a hardware xor accelerator  an advantage of this model over the pure software raid is that if using a redundancy mode the boot drive is protected from failure  due to the firmware  during the boot process even before the operating systems drivers take over       data scrubbing  referred to in some environments as patrol read  involves periodic reading and checking by the raid controller of all the blocks in an array  including those not otherwise accessed  this detects bad blocks before use    data scrubbing checks for bad blocks on each storage device in an array  but also uses the redundancy of the array to recover bad blocks on a single drive and to reassign the recovered data to spare blocks elsewhere on the drive       frequently  a raid controller is configured to  drop  a component drive  that is  to assume a component drive has failed  if the drive has been unresponsive for eight seconds or so  this might cause the array controller to drop a good drive because that drive has not been given enough time to complete its internal error recovery procedure  consequently  using raid for consumer marketed drives can be risky  and so called  enterprise class  drives limit this error recovery time to reduce risk citation needed western digital s desktop drives used to have a specific fix  a utility called wdtler exe limited a drive s error recovery time  the utility enabled tler  time limited error recovery   which limits the error recovery time to seven seconds  around september       western digital disabled this feature in their desktop drives  e g  the caviar black line   making such drives unsuitable for use in raid configurations    however  western digital enterprise class drives are shipped from the factory with tler enabled  similar technologies are used by seagate  samsung  and hitachi  of course  for non raid usage  an enterprise class drive with a short error recovery timeout that cannot be changed is therefore less suitable than a desktop drive    in late       the smartmontools program began supporting the configuration of ata error recovery control  allowing the tool to configure many desktop class hard drives for use in raid setups       while raid may protect against physical drive failure  the data is still exposed to operator  software  hardware  and virus destruction  many studies cite operator fault as the most common source of malfunction    such as a server operator replacing the incorrect drive in a faulty raid  and disabling the system  even temporarily  in the process       an array can be overwhelmed by catastrophic failure that exceeds its recovery capacity and  of course  the entire array is at risk of physical damage by fire  natural disaster  and human forces  while backups can be stored off site  an array is also vulnerable to controller failure because it is not always possible to migrate it to a new  different controller without data loss       in practice  the drives are often the same age  with similar wear  and subject to the same environment  since many drive failures are due to mechanical issues  which are more likely on older drives   this violates the assumptions of independent  identical rate of failure amongst drives  failures are in fact statistically correlated    in practice  the chances of a second failure before the first has been recovered  causing data loss  is higher than four random failures  in a study of about         drives  the probability of two drives in the same cluster failing within one hour was four times larger than predicted by the exponential statistical distribution which characterizes processes in which events occur continuously and independently at a constant average rate  the probability of two failures in the same    hour period was twice as large as predicted by an exponential distribution       unrecoverable read errors  ure  present as sector read failures  also known as latent sector errors  lse   the associated media assessment measure  unrecoverable bit error  ube  rate  is typically specified at one bit in      for enterprise class drives  scsi  fc or sas   and one bit in      for desktop class drives  ide ata pata or sata   increasing drive capacities and large raidxa   instances have led to an increasing inability to successfully rebuild a raid set after a drive failure and occurrence of an unrecoverable sector on the remaining drives      when rebuilding  parity based schemes such as raidxa   are particularly prone to the effects of ures as they affect not only the sector where they occur  but also reconstructed blocks using that sector for parity computation  thus  an ure during a raidxa   rebuild typically leads to a complete rebuild failure       double protection parity based schemes  such as raidxa    attempt to address this issue by providing redundancy that allows double drive failures  as a downside  such schemes suffer from elevated write penalty  schemes that duplicate  mirror  data in a drive to drive manner  such as raidxa   and raidxa     have a lower risk from ures than those using parity computation or mirroring between striped sets      data scrubbing  as a background process  can be used to detect and recover from ures  effectively reducing the risk of them happening during raid rebuilds and causing double drive failures  the recovery of ures involves remapping of affected underlying disk sectors  utilizing the drive s sector remapping pool  in case of ures detected during background scrubbing  data redundancy provided by a fully operational raid set allows the missing data to be reconstructed and rewritten to a remapped sector         drive capacity has grown at a much faster rate than transfer speed  and error rates have only fallen a little in comparison  therefore  larger capacity drives may take hours  if not days  to rebuild  the rebuild time is also limited if the entire array is still in operation at reduced capacity    given an array with only one drive of redundancy  raids       and     a second failure would cause complete failure of the array  even though individual drives  mean time between failure  mtbf  have increased over time  this increase has not kept pace with the increased storage capacity of the drives  the time to rebuild the array after a single drive failure  as well as the chance of a second failure during a rebuild  have increased over time       some commentators have declared that raidxa   is only a  band aid  in this respect  because it only kicks the problem a little further down the road    however  according to a      netapp study of berriman et al   the chance of failure decreases by a factor of about        relative to raidxa    for a proper implementation of raidxa    even when using commodity drives    nevertheless  if the currently observed technology trends remain unchanged  in      a raidxa   array will have the same chance of failure as its raidxa   counterpart had in              mirroring schemes such as raidxa    have a bounded recovery time as they require the copy of a single failed drive  compared with parity schemes such as raidxa    which require the copy of all blocks of the drives in an array set  triple parity schemes  or triple mirroring  have been suggested as one approach to improve resilience to an additional drive failure during this large rebuild time       a system crash or other interruption of a write operation can result in states where the parity is inconsistent with the data due to non atomicity of the write process  such that the parity cannot be used for recovery in the case of a disk failure  the so called raid   write hole     the raid write hole is a known data corruption issue in older and low end raids  caused by interrupted destaging of writes to disk       this is a little understood and rarely mentioned failure mode for redundant storage systems that do not utilize transactional features  database researcher jim gray wrote  update in place is a poison apple  during the early days of relational database commercialization       a concern about write cache reliability exists  specifically regarding devices equipped with a write back cache a caching system that reports the data as written as soon as it is written to cache  as opposed to the non volatile medium    