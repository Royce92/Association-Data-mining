 in mathematics  computer science  economics  or management science  mathematical optimization  alternatively  optimization or mathematical programming  is the selection of a best element  with regard to some criteria  from some set of available alternatives      in the simplest case  an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function  the generalization of optimization theory and techniques to other formulations comprises a large area of applied mathematics  more generally  optimization includes finding  best available  values of some objective function given a defined domain  or a set of constraints   including a variety of different types of objective functions and different types of domains             an optimization problem can be represented in the following way     such a formulation is called an optimization problem or a mathematical programming problem  a term not directly related to computer programming  but still in use for example in linear programming   see history below   many real world and theoretical problems may be modeled in this general framework  problems formulated using this technique in the fields of physics and computer vision may refer to the technique as energy minimization  speaking of the value of the function f as representing the energy of the system being modeled     typically  a is some subset of the euclidean space rn  often specified by a set of constraints  equalities or inequalities that the members of a have to satisfy  the domain a of f is called the search space or the choice set  while the elements of a are called candidate solutions or feasible solutions     the function f is called  variously  an objective function  a loss function or cost function  minimization    a utility function  maximization   a fitness function  maximization   or  in certain fields  an energy function  or energy functional  a feasible solution that minimizes  or maximizes  if that is the goal  the objective function is called an optimal solution     by convention  the standard form of an optimization problem is stated in terms of minimization  generally  unless both the objective function and the feasible region are convex in a minimization problem  there may be several local minima  where a local minimum x  is defined as a point for which there exists some       so that for all x such that    the expression    holds  that is to say  on some region around x  all of the function values are greater than or equal to the value at that point  local maxima are defined similarly     a large number of algorithms proposed for solving non convex problems   including the majority of commercially available solvers   are not capable of making a distinction between local optimal solutions and rigorous optimal solutions  and will treat the former as actual solutions to the original problem  the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a non convex problem is called global optimization     optimization problems are often expressed with special notation  here are some examples     consider the following notation     this denotes the minimum value of the objective function   when choosing x from the set of real numbers   the minimum value in this case is   occurring at      similarly  the notation    asks for the maximum value of the objective function  x  where x may be any real number  in this case  there is no such maximum as the objective function is unbounded  so the answer is  infinity  or  undefined      consider the following notation     or equivalently    this represents the value  or values  of the argument x in the interval  that minimizes  or minimize  the objective function x xa  xa    the actual minimum value of that function is not what the problem asks for   in this case  the answer is x       since x     is infeasible  i e  does not belong to the feasible set     similarly     or equivalently    represents the  pair  or pairs  that maximizes  or maximize  the value of the objective function   with the added constraint that x lie in the interval   again  the actual maximum value of the expression does not matter   in this case  the solutions are the pairs of the form      k   and       k       where k ranges over all integers     arg min and arg max are sometimes also written argmin and argmax  and stand for argument of the minimum and argument of the maximum     fermat and lagrange found calculus based formulas for identifying optima  while newton and gauss proposed iterative methods for moving towards an optimum  historically  the first term for optimization was  linear programming   which was due to georgexa b  dantzig  although much of the theory had been introduced by leonid kantorovich in       dantzig published the simplex algorithm in       and john von neumann developed the theory of duality in the same year     the term  programming  in this context does not refer to computer programming  rather  the term comes from the use of program by the united states military to refer to proposed training and logistics schedules  which were the problems dantzig studied at that time     later important researchers in mathematical optimization include the following     in a number of subfields  the techniques are designed primarily for optimization in dynamic contexts  that is  decision making over time      adding more than one objective to an optimization problem adds complexity  for example  to optimize a structural design  one would want a design that is both light and rigid  because these two objectives conflict  a trade off exists  there will be one lightest design  one stiffest design  and an infinite number of designs that are some compromise of weight and stiffness  the set of trade off designs that cannot be improved upon according to one criterion without hurting another criterion is known as the pareto set  the curve created plotting weight against stiffness of the best designs is known as the pareto frontier     a design is judged to be  pareto optimal   equivalently   pareto efficient  or in the pareto set  if it is not dominated by any other design  if it is worse than another design in some respects and no better in any respect  then it is dominated and is not pareto optimal     the choice among  pareto optimal  solutions to determine the  favorite solution  is delegated to the decision maker  in other words  defining the problem as multiobjective optimization signals that some information is missing  desirable objectives are given but not their detailed combination  in some cases  the missing information can be derived by interactive sessions with the decision maker     multi objective optimization problems have been generalized further to vector optimization problems where the  partial  ordering is no longer given by the pareto ordering     optimization problems are often multi modal  that is  they possess multiple good solutions  they could all be globally good  same cost function value  or there could be a mix of globally good and locally good solutions  obtaining all  or at least some of  the multiple solutions is the goal of a multi modal optimizer     classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions  since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm  evolutionary algorithms are however a very popular approach to obtain multiple solutions in a multi modal optimization task     the satisfiability problem  also called the feasibility problem  is just the problem of finding any feasible solution at all without regard to objective value  this can be regarded as the special case of mathematical optimization where the objective value is the same for every solution  and thus any solution is optimal     many optimization algorithms need to start from a feasible point  one way to obtain such a point is to relax the feasibility conditions using a slack variable  with enough slack  any starting point is feasible  then  minimize that slack variable until slack is null or negative     the extreme value theorem of karl weierstrass states that a continuous real valued function on a compact set attains its maximum and minimum value  more generally  a lower semi continuous function on a compact set attains its minimum  an upper semi continuous function on a compact set attains its maximum     one of fermat s theorems states that optima of unconstrained problems are found at stationary points  where the first derivative or the gradient of the objective function is zero  see first derivative test   more generally  they may be found at critical points  where the first derivative or gradient of the objective function is zero or is undefined  or on the boundary of the choice set  an equation  or set of equations  stating that the first derivative s  equal s  zero at an interior optimum is called a  first order condition  or a set of first order conditions     optima of equality constrained problems can be found by the lagrange multiplier method  the optima of problems with equality and or inequality constraints can be found using the  karush kuhn tucker conditions      while the first derivative test identifies points that might be extrema  this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither  when the objective function is twice differentiable  these cases can be distinguished by checking the second derivative or the matrix of second derivatives  called the hessian matrix  in unconstrained problems  or the matrix of second derivatives of the objective function and the constraints called the bordered hessian in constrained problems  the conditions that distinguish maxima  or minima  from other stationary points are called  second order conditions   see  second derivative test    if a candidate solution satisfies the first order conditions  then satisfaction of the second order conditions as well is sufficient to establish at least local optimality     the envelope theorem describes how the value of an optimal solution changes when an underlying parameter changes  the process of computing this change is called comparative statics     the maximum theorem of claude berge        describes the continuity of an optimal solution as a function of underlying parameters     for unconstrained problems with twice differentiable functions  some critical points can be found by finding the points where the gradient of the objective function is zero  that is  the stationary points   more generally  a zero subgradient certifies that a local minimum has been found for minimization problems with convex functions and other locally lipschitz functions     further  critical points can be classified using the definiteness of the hessian matrix  if the hessian is positive definite at a critical point  then the point is a local minimum  if the hessian matrix is negative definite  then the point is a local maximum  finally  if indefinite  then the point is some kind of saddle point     constrained problems can often be transformed into unconstrained problems with the help of lagrange multipliers  lagrangian relaxation can also provide approximate solutions to difficult constrained problems     when the objective function is convex  then any local minimum will also be a global minimum  there exist efficient numerical techniques for minimizing convex functions  such as interior point methods     to solve problems  researchers may use algorithms that terminate in a finite number of steps  or iterative methods that converge to a solution  on some specified class of problems   or heuristics that may provide approximate solutions to some problems  although their iterates need not converge      the iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate hessians  gradients  or only function values  while evaluating hessians  h  and gradients  g  improves the rate of convergence  for functions for which these quantities exist and vary sufficiently smoothly  such evaluations increase the computational complexity  or computational cost  of each iteration  in some cases  the computational complexity may be excessively high     one major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort  usually much more effort than within the optimizer itself  which mainly has to operate over the n variables  the derivatives provide detailed information for such optimizers  but are even harder to calculate  e g  approximating the gradient takes at least n   function evaluations  for approximations of the  nd derivatives  collected in the hessian matrix  the number of function evaluations is in the order of n   newton s method requires the  nd order derivates  so for each iteration the number of function calls is in the order of n   but for a simpler pure gradient optimizer it is only n  however  gradient optimizers need usually more iterations than newton s algorithm  which one is best with respect to the number of function calls depends on the problem itself     more generally  if the objective function is not a quadratic function  then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution  the first and still popular method for ensuring convergence relies on line searches  which optimize a function along one dimension  a second and increasingly popular method for ensuring convergence uses trust regions  both line searches and trust regions are used in modern methods of non differentiable optimization  usually a global optimizer is much slower than advanced local optimizers  such as bfgs   so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points     besides  finitely terminating  algorithms and  convergent  iterative methods  there are heuristics that can provide approximate solutions to some optimization problems     problems in rigid body dynamics  in particular articulated rigid body dynamics  often require mathematical programming techniques  since you can view rigid body dynamics as attempting to solve an ordinary differential equation on a constraint manifold  the constraints are various nonlinear geometric constraints such as  these two points must always coincide    this surface must not penetrate any other   or  this point must always lie somewhere on this curve   also  the problem of computing contact forces can be done by solving a linear complementarity problem  which can also be viewed as a qp  quadratic programming  problem     many design problems can also be expressed as optimization programs  this application is called design optimization  one subset is the engineering optimization  and another recent and growing subset of this field is multidisciplinary design optimization  which  while useful in many problems  has in particular been applied to aerospace engineering problems     economics is closely enough linked to optimization of agents that an influential definition relatedly describes economics qua science as the  study of human behavior as a relationship between ends and scarce means  with alternative uses   modern optimization theory includes traditional optimization theory but also overlaps with game theory and the study of economic equilibria  the journal of economic literature codes classify mathematical programming  optimization techniques  and related topics under jel c   c       in microeconomics  the utility maximization problem and its dual problem  the expenditure minimization problem  are economic optimization problems  insofar as they behave consistently  consumers are assumed to maximize their utility  while firms are usually assumed to maximize their profit  also  agents are often modeled as being risk averse  thereby preferring to avoid risk  asset prices are also modeled using optimization theory  though the underlying mathematics relies on optimizing stochastic processes rather than on static optimization  trade theory also uses optimization to explain trade patterns between nations  the optimization of market portfolios is an example of multi objective optimization in economics     since the     s  economists have modeled dynamic decisions over time using control theory  for example  microeconomists use dynamic search models to study labor market behavior   a crucial distinction is between deterministic and stochastic models   macroeconomists build dynamic stochastic general equilibrium  dsge  models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers  consumers  investors  and governments       another field that uses optimization techniques extensively is operations research   operations research also uses stochastic modeling and simulation to support improved decision making  increasingly  operations research uses stochastic programming to model dynamic decisions that adapt to events  such problems can be solved with large scale optimization and stochastic optimization methods     mathematical optimization is used in much modern controller design  high level controllers such as model predictive control  mpc  or real time optimization  rto  employ mathematical optimization  these algorithms run online and repeatedly determine values for decision variables  such as choke openings in a process plant  by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled     nonlinear optimization methods are used to construct computational models of oil reservoirs       nonlinear optimization methods are widely used in conformational analysis     methods to obtain suitable  in some sense  natural extensions of optimization problems that otherwise lack of existence or stability of solutions to obtain problems with guaranteed existence of solutions and their stability in some sense  typically under various perturbation of data  are in general called relaxation  solutions of such extended   relaxed  problems in some sense characterizes  at least certain features  of the original problems  e g  as far as their optimizing sequences concerns  relaxed problems may also possesses their own natural linear structure that may yield specific optimality conditions different from optimality conditions for the original problems  