 vmware esxi  formerly esx  is an enterprise class  type   hypervisor developed by vmware for deploying and serving virtual computers  as a type   hypervisor  esxi is not a software application that one installs in an operating system  instead  it includes and integrates vital os components  such as a kernel      after version      vmware renamed esx to esxi  esxi replaces service console  a rudimentary operating system  with a more closely integrated os  esx esxi is the primary component in the vmware infrastructure software suite      the name esx is an abbreviation of elastic sky x               esx runs on bare metal  without running an operating system   unlike other vmware products   it includes its own kernel  a linux kernel is started first   and is then used to load a variety of specialized virtualization components  including esx  which is otherwise known as the vmkernel component citation needed the linux kernel is the primary virtual machine  it is invoked by the service console  at normal run time  the vmkernel is running on the bare computer  and the linux based service console runs as the first virtual machine  vmware dropped development of esx at version      and now uses esxi  which does not include a linux kernel citation needed    the vmkernel is a microkernel  with three interfaces  hardware  guest systems  and the service console  console os      the vmkernel handles cpu and memory directly  using scan before execution  sbe  to handle special or privileged cpu instructions     and the srat  system resource allocation table  to track allocated memory       access to other hardware  such as network or storage devices  takes place using modules  at least some of the modules derive from modules used in the linux kernel  to access these modules  an additional module called vmklinux implements the linux module interface  according to the readme file   this module contains the linux emulation layer used by the vmkernel        the vmkernel uses the device drivers       these drivers mostly equate to those described in vmware s hardware compatibility list    all these modules fall under the gpl  programmers have adapted them to run with the vmkernel  vmware inc has changed the module loading and some other minor things       in esx  and not esxi   the service console is a vestigial general purpose operating system most significantly used as bootstrap for the vmware kernel  vmkernel  and secondarily used as a management interface  both of these console operating system functions are being deprecated from version      as vmware migrates exclusively to the esxi model  current version being esxi    the service console  for all intents and purposes  is the operating system used to interact with vmware esx and the virtual machines that run on the server     esx uses a linux kernel to load additional code  often referred to by vmware  inc  as the  vmkernel   the dependencies between the  vmkernel  and the linux part of the esx server have changed drastically over different major versions of the software  the vmware faq   states   esx server also incorporates a service console based on a linux     kernel that is used to boot the esx server virtualization layer   the linux kernel runs before any other software on an esx host   on esx versions   and    no vmkernel processes run on the system during the boot process    after the linux kernel has loaded  the s  vmware script loads the vmkernel    vmware inc states that vmkernel does not derive from linux  but acknowledges that it has adapted certain device drivers from linux device drivers  the linux kernel continues running  under the control of the vmkernel  providing functions including the proc file system used by the esx and an environment to run support applications    esx version   loads the vmkernel from the linux initrd  thus much earlier in the boot sequence than in previous esx versions     in traditional systems  a given operating system runs a single kernel  the vmware faq mentions that esx has both a linux     kernel and vmkernel   hence confusion over whether esx has a linux base  an esx system starts a linux kernel first  but it loads vmkernel  also described by vmware as a kernel   which according to vmware  wraps around  the linux kernel  and which  according to vmware inc  does not derive from linux     the esx userspace environment  known as the  service console   or as  cos  or as  vmnix    derives from a modified version of red hat linux   red hat     for esx   x and red hat enterprise linux   for esx   x   in general  this service console provides management interfaces  cli  webpage mui  remote console      as a further detail which differentiates the esx from other vmware virtualization products  esx supports the vmware proprietary cluster file system vmfs  vmfs enables multiple hosts to access the same san luns simultaneously  while file level locking provides simple protection to file system integrity     in the event of a hardware error  the vmkernel can  catch  a machine check exception    this results in an error message displayed on a purple diagnostic screen  this is colloquially known as a purple diagnostic screen  or purple screen of death  psod  cf  blue screen of death  bsod       upon displaying a purple diagnostic screen  the vmkernel writes debug information to the core dump partition  this information  together with the error codes displayed on the purple diagnostic screen can be used by vmware support to determine the cause of the problem     live migration  vmotion  in esx allows a virtual machine to move between two different hosts  live storage migration  storage vmotion  enables live migration of virtual disks on the fly    during vmotion live migration  vlm  of a running virtual machine  vm  the content of the  ram  memory of the vm is sent from the running vm to the new vm  the instance on another host that will become the running vm after the vlm   the content of memory is by its nature changing all the time  esx uses a system where the content is sent to the other vm and then it will check what data is changed and send that  each time smaller blocks  at the last moment it will very briefly  freeze  the existing vm  transfer the last changes in the ram content and then start the new vm  the intended effect of this process is to minimize the time during which the vm is suspended  in a best case this will be the time of the final transfer plus the time required to start the new vm         vmware esx is available in two main types  esx and esxi  although since version   only esxi is continued     version release history     vmware esx        vmware esx         july          vmware esx         december          vmware infrastructure      vi      june          vmware vsphere         may          esx and esxi before version     do not support windows   windows       these microsoft operating systems can only run on esxi   x or later          july      vsphere     and its subsequent update and patch releases are the last releases to include both esx and esxi hypervisor architectures  future major releases of vmware vsphere will include only the vmware esxi architecture  for this reason  vmware recommends that deployments of vsphere   x utilize the esxi hypervisor architecture     vmware esxi is a smaller footprint version of esx that does not include the esx service console  it is available without the need to purchase a vcenter license as a free download from vmware with some features disabled           vmware esxi was originally a compact version of vmware esx that allowed for a smaller    mb disk footprint on the host  with a simple configuration console for mostly network configuration and remote based vmware infrastructure client interface  this allows for more resources to be dedicated to the guest environments     there are two variations of esxi  vmware esxi installable and vmware esxi embedded edition  the same installation media will install to either one or the other of these installation modes depending on the size of the target media    it has the ability to upgrade to vmware infrastructure     or vmware vsphere     esxi     originally named vmware esx server esxi edition  through several revisions the product finally became vmware esxi    new editions then followed  esxi      esxi   and now esxi       to virtualize windows   or windows server      as guest operating systems  the esxi version must be     update   or later     version release history       the following products operate in conjunction with esx     network connectivity between esx hosts and the vm s running on it relies on virtual nic s  inside the vm  and virtual switches  the latter exists in two versions  the  standard  vswitch allowing several vm s on a single esx host to share a physical nic and the  distributed vswitch  where the vswitches on different esx hosts together form one logical switch  cisco offers in their cisco nexus product line the nexus     v  an advanced version of the standard distributed vswitch  a nexus     v consists of two parts  a supervisor module  vsm  and on each esx host a virtual ethernet module  vem   the vsm runs as a virtual appliance within the esx cluster or on dedicated hardware  nexus      series  and the vem runs as module on each host and replaces a standard dvs  distributed virtual switch  from vmware  configuration of the switch is done on the vsm using the standard nx os cli  it offers capabilities to create standard port profiles which can then be assigned to virtual machines using vcenter     there are several differences between the standard dvs and the n    v  one is that the cisco switch generally has full support for network technologies such as lacp link aggregation or that the vmware switch supports new features such as routing based on physical nic load  however the main difference lies in the architecture  nexus     v is working in the same way as a physical ethernet switch does while dvs is relying on information from esx  this has consequences for example in scalability where the limit for a n    v is      virtual ports against       for a dvs  the nexus    v is developed in co operation between cisco and vmware and uses the api of the dvs      because vmware esx is a leader in the server virtualisation market    software and hardware vendors offer a range of tools to integrate their products or services with esx  examples are the products from veeam software with backup and management applications   and a plugin to monitor and manage esx using hp openview    quest software with a range of management and backup applications and most major backup solution providers have plugins or modules for esx  using microsoft operations manager  scom            with a bridgeways esx management pack gives you a realtime esx datacenter health view     also hardware vendors such as hp and dell include tools to support the use of esx i  on their hardware platforms  an example is the esx module for dell s openmanage management platform   nvmware have added a web client   since v  but it will work on vcenter only and does not contain all features    veman   is a linux application which is trying to fill that gap  these are just a few examples  there are numerous  rd party products to manage  monitor or backup esx infrastructures and the vms running on them      known limitations of vmware esxi  as of june       include the following     some maximums in esxi server     may influence the design of data centers         in terms of performance  virtualization imposes a cost in the additional work the cpu has to perform to virtualize the underlying hardware  instructions that perform this extra work  and other activities that require virtualization  tend to lie in operating system calls  in an unmodified operating system  os calls introduce the greatest portion of virtualization  overhead  citation needed    paravirtualization or other virtualization techniques may help with these issues  vmware developed the virtual machine interface for this purpose  and selected operating systems currentlyupdate support this  a comparison between full virtualization and paravirtualization for the esx server   shows that in some cases paravirtualization is much faster     when using the advanced and extended network capabilities by using the cisco nexus     v distributed virtual switch the following network related limitations apply    