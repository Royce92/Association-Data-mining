 peer to peer  p p  computing or networking is a distributed application architecture that partitions tasks or work loads between peers  peers are equally privileged  equipotent participants in the application  they are said to form a peer to peer network of nodes     peers make a portion of their resources  such as processing power  disk storage or network bandwidth  directly available to other network participants  without the need for central coordination by servers or stable hosts   peers are both suppliers and consumers of resources  in contrast to the traditional client server model in which the consumption and supply of resources is divided  emerging collaborative p p systems are going beyond the era of peers doing similar things while sharing resources  and are looking for diverse peers that can bring in unique resources and capabilities to a virtual community thereby empowering it to engage in greater tasks beyond those that can be accomplished by individual peers  yet that are beneficial to all the peers      while p p systems had previously been used in many application domains   the architecture was popularized by the file sharing system napster  originally released in       the concept has inspired new structures and philosophies in many areas of human interaction  in such social contexts  peer to peer as a meme refers to the egalitarian social networking that has emerged throughout society  enabled by internet technologies in general             while p p systems had previously been used in many application domains   the concept was popularized by file sharing systems such as the music sharing application napster  originally released in       citation needed the peer to peer movement allowed millions of internet users to connect  directly  forming groups and collaborating to become user created search engines  virtual supercomputers  and filesystems     the basic concept of peer to peer computing was envisioned in earlier software systems and networking discussions  reaching back to principles stated in the first request for comments  rfc        tim berners lee s vision for the world wide web was close to a p p network in that it assumed each user of the web would be an active editor and contributor  creating and linking content to form an interlinked  web  of links  the early internet was more open than present day  where two machines connected to the internet could send packets to each other without firewalls and other security measures   this contrasts to the broadcasting like structure of the web as it has developed over the years   as a precursor to the internet  arpanet was a successful client server network where  every participating node could request and serve content   however  arpanet was not self organized and it lacked the ability to  provide any means for context or content based routing beyond  simple  addressed based routing       therefore  a distributed messaging system that is often likened as an early peer to peer architecture was established  usenet  usenet was developed in      and is a system that enforces a decentralized model of control  the basic model is a client server model from the user or client perspective that offers a self organizing approach to newsgroup servers  however  news servers communicate with one another as peers to propagate usenet news articles over the entire group of network servers  the same consideration applies to smtp email in the sense that the core email relaying network of mail transfer agents has a peer to peer character  while the periphery of e mail clients and their direct connections is strictly a client server relationship citation needed    in may       with millions more people on the internet  shawn fanning introduced the music and file sharing application called napster   napster was the beginning of peer to peer networks  as we know them today  where  participating users establish a virtual network  entirely independent from the physical network  without having to obey any administrative authorities or restrictions       a peer to peer network is designed around the notion of equal peer nodes simultaneously functioning as both  clients  and  servers  to the other nodes on the network  this model of network arrangement differs from the client server model where communication is usually to and from a central server  a typical example of a file transfer that uses the client server model is the file transfer protocol  ftp  service in which the client and server programs are distinct  the clients initiate the transfer  and the servers satisfy these requests     peer to peer networks generally implement some form of virtual overlay network on top of the physical network topology  where the nodes in the overlay form a subset of the nodes in the physical network  data is still exchanged directly over the underlying tcp ip network  but at the application layer peers are able to communicate with each other directly  via the logical overlay links  each of which corresponds to a path through the underlying physical network   overlays are used for indexing and peer discovery  and make the p p system independent from the physical network topology  based on how the nodes are linked to each other within the overlay network  and how resources are indexed and located  we can classify networks as unstructured or structured  or as a hybrid between the two          unstructured peer to peer networks do not impose a particular structure on the overlay network by design  but rather are formed by nodes that randomly form connections to each other     gnutella  gossip  and kazaa are examples of unstructured p p protocols        because there is no structure globally imposed upon them  unstructured networks are easy to build and allow for localized optimizations to different regions of the overlay    also  because the role of all peers in the network is the same  unstructured networks are highly robust in the face of high rates of  churn  that is  when large numbers of peers are frequently joining and leaving the network         however the primary limitations of unstructured networks also arise from this lack of structure  in particular  when a peer wants to find a desired piece of data in the network  the search query must be flooded through the network to find as many peers as possible that share the data  flooding causes a very high amount of signaling traffic in the network  uses more cpu memory  by requiring every peer to process all search queries   and does not ensure that search queries will always be resolved  furthermore  since there is no correlation between a peer and the content managed by it  there is no guarantee that flooding will find a peer that has the desired data  popular content is likely to be available at several peers and any peer searching for it is likely to find the same thing  but if a peer is looking for rare data shared by only a few other peers  then it is highly unlikely that search will be successful       in structured peer to peer networks the overlay is organized into a specific topology  and the protocol ensures that any node can efficiently   search the network for a file resource  even if the resource is extremely rare     the most common type of structured p p networks implement a distributed hash table  dht       in which a variant of consistent hashing is used to assign ownership of each file to a particular peer      this enables peers to search for resources on the network using a hash table  that is   key  value  pairs are stored in the dht  and any participating node can efficiently retrieve the value associated with a given key         however  in order to route traffic efficiently through the network  nodes in a structured overlay must maintain lists of neighbors that satisfy specific criteria  this makes them less robust in networks with a high rate of churn  i e  with large numbers of nodes frequently joining and leaving the network       more recent evaluation of p p resource discovery solutions under real workloads have pointed out several issues in dht based solutions such as high cost of advertising discovering resources and static and dynamic load imbalance       notable distributed networks that use dhts include bittorrent s distributed tracker  the kad network  the storm botnet  yacy  and the coral content distribution network  some prominent research projects include the chord project  kademlia  past storage utility  p grid  a self organized and emerging overlay network  and coopnet content distribution system citation needed dht based networks have also been widely utilized for accomplishing efficient resource discovery     for grid computing systems  as it aids in resource management and scheduling of applications     hybrid models are a combination of peer to peer and client server models    a common hybrid model is to have a central server that helps peers find each other  spotify is an example of a hybrid model  there are a variety of hybrid models  all of which make trade offs between the centralized functionality provided by a structured server client network and the node equality afforded by the pure peer to peer unstructured networks  currently  hybrid models have better performance than either pure unstructured networks or pure structured networks because certain functions  such as searching  do require a centralized functionality but benefit from the decentralized aggregation of nodes provided by unstructured networks       peer to peer systems pose unique challenges from a computer security perspective     like any other form of software  p p applications can contain vulnerabilities  what makes this particularly dangerous for p p software  however  is that peer to peer applications act as servers as well as clients  meaning that they can be more vulnerable to remote exploits       also  since each node plays a role in routing traffic through the network  malicious users can perform a variety of  routing attacks   or denial of service attacks  examples of common routing attacks include  incorrect lookup routing  whereby malicious nodes deliberately forward requests incorrectly or return false results   incorrect routing updates  where malicious nodes corrupt the routing tables of neighboring nodes by sending them false information  and  incorrect routing network partition  where when new nodes are joining they bootstrap via a malicious node  which places the new node in a partition of the network that is populated by other malicious nodes       the prevalence of malware varies between different peer to peer protocols  studies analyzing the spread of malware on p p networks found  for example  that     of the answered download requests on the limewire network contained some form of malware  whereas only    of the content on openft contained malware  in both cases  the top three most common types of malware accounted for the large majority of cases      in limewire  and     in openft   another study analyzing traffic on the kazaa network found that     of the         file sample taken were infected by one or more of the     different computer viruses that were tested for       corrupted data can also be distributed on p p networks by modifying files that are already being shared on the network  for example  on the fasttrack network  the riaa managed to introduce faked chunks into downloads and downloaded files  mostly mp  files   files infected with the riaa virus were unusable afterwards and contained malicious code  the riaa is also known to have uploaded fake music and movies to p p networks in order to deter illegal file sharing    consequently  the p p networks of today have seen an enormous increase of their security and file verification mechanisms  modern hashing  chunk verification and different encryption methods have made most networks resistant to almost any type of attack  even when major parts of the respective network have been replaced by faked or nonfunctional hosts       the decentralized nature of p p networks increases robustness because it removes the single point of failure that can be inherent in a client server based system    as nodes arrive and demand on the system increases  the total capacity of the system also increases  and the likelihood of failure decreases  if one peer on the network fails to function properly  the whole network is not compromised or damaged  in contrast  in a typical client server architecture  clients share only their demands with the system  but not their resources  in this case  as more clients join the system  fewer resources are available to serve each client  and if the central server fails  the entire network is taken down     there are both advantages and disadvantages in p p networks related to the topic of data backup  recovery  and availability  in a centralized network  the system administrators are the only forces controlling the availability of files being shared  if the administrators decide to no longer distribute a file  they simply have to remove it from their servers  and it will no longer be available to users  along with leaving the users powerless in deciding what is distributed throughout the community  this makes the entire system vulnerable to threats and requests from the government and other large forces  for example  youtube has been pressured by the riaa  mpaa  and entertainment industry to filter out copyrighted content  although server client networks are able to monitor and manage content availability  they can have more stability in the availability of the content they choose to host  a client should not have trouble accessing obscure content that is being shared on a stable centralized network  p p networks  however  are more unreliable in sharing unpopular files because sharing files in a p p network requires that at least one node in the network has the requested data  and that node must be able to connect to the node requesting the data  this requirement is occasionally hard to meet because users may delete or stop sharing data at any point       in this sense  the community of users in a p p network is completely responsible for deciding what content is available  unpopular files will eventually disappear and become unavailable as more people stop sharing them  popular files  however  will be highly and easily distributed  popular files on a p p network actually have more stability and availability than files on central networks  in a centralized network a simple loss of connection between the server and clients is enough to cause a failure  but in p p networks the connections between every node must be lost in order to cause a data sharing failure  in a centralized system  the administrators are responsible for all data recovery and backups  while in p p systems  each node requires its own backup system  because of the lack of central authority in p p networks  forces such as the recording industry  riaa  mpaa  and the government are unable to delete or stop the sharing of content on p p systems       in p p networks  clients both provide and use resources  this means that unlike client server systems  the content serving capacity of peer to peer networks can actually increase as more users begin to access the content  especially with protocols such as bittorrent that require users to share  refer a performance measurement study     this property is one of the major advantages of using p p networks because it makes the setup and running costs very small for the original content distributor         many file peer to peer file sharing networks  such as gnutella  g   and the edonkey network popularized peer to peer technologies     peer to peer networking involves data transfer from one user to another without using an intermediate server  companies developing p p applications have been involved in numerous legal cases  primarily in the united states  over conflicts with copyright law    two major cases are grokster vs riaa and mgm studios  inc  v  grokster  ltd     in both of the cases the file sharing technology was ruled to be legal as long as the developers had no ability to prevent the sharing of the copyrighted material citation needed    cooperation among a community of participants is key to the continued success of p p systems aimed at casual human users  these reach their full potential only when large numbers of nodes contribute resources  but in current practice p p networks often contain large numbers of users who utilize resources shared by other nodes  but who do not share anything themselves  often referred to as the  freeloader problem    freeloading can have a profound impact on the network and in some cases can cause the community to collapse    in these types of networks  users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance      studying the social attributes of p p networks is challenging due to large populations of turnover  asymmetry of interest and zero cost identity    a variety of incentive mechanisms have been implemented to encourage or even force nodes to contribute resources       some researchers have explored the benefits of enabling virtual communities to self organize and introduce incentives for resource sharing and cooperation  arguing that the social aspect missing from today s p p systems should be seen both as a goal and a means for self organized virtual communities to be built and fostered    ongoing research efforts for designing effective incentive mechanisms in p p systems  based on principles from game theory  are beginning to take on a more psychological and information processing direction     some peer to peer networks  e g  freenet  place a heavy emphasis on privacy and anonymity that is  ensuring that the contents of communications are hidden from eavesdroppers  and that the identities locations of the participants are concealed  public key cryptography can be used to provide encryption  data validation  authorization  and authentication for data messages  onion routing and other mix network protocols  e g  tarzan  can be used to provide anonymity       although peer to peer networks can be used for legitimate purposes  rights holders have targeted peer to peer over the involvement with sharing copyrighted material  peer to peer networking involves data transfer from one user to another without using an intermediate server  companies developing p p applications have been involved in numerous legal cases  primarily in the united states  primarily over issues surrounding copyright law    two major cases are grokster vs riaa and mgm studios  inc  v  grokster  ltd     in both of the cases the file sharing technology was ruled to be legal as long as the developers had no ability to prevent the sharing of the copyrighted material  to establish criminal liability for the copyright infringement on peer to peer systems  the government must prove that the defendant infringed a copyright willingly for the purpose of personal financial gain or commercial advantage    fair use exceptions allow limited use of copyrighted material to be downloaded without acquiring permission from the rights holders  these documents are usually news reporting or under the lines of research and scholarly work  controversies have developed over the concern of illegitimate use of peer to peer networks regarding public safety and national security  when a file is downloaded through a peer to peer network  it is impossible to know who created the file or what users are connected to the network at a given time  trustworthiness of sources is a potential security threat that can be seen with peer to peer systems       peer to peer applications present one of the core issues in the network neutrality controversy  internet service providers  isps  have been known to throttle p p file sharing traffic due to its high bandwidth usage    compared to web browsing  e mail or many other uses of the internet  where data is only transferred in short intervals and relative small quantities  p p file sharing often consists of relatively heavy bandwidth usage due to ongoing file transfers and swarm network coordination packets  in october       comcast  one of the largest broadband internet providers in the usa  started blocking p p applications such as bittorrent  their rationale was that p p is mostly used to share illegal content  and their infrastructure is not designed for continuous  high bandwidth traffic  critics point out that p p networking has legitimate legal uses  and that this is another way that large providers are trying to control use and content on the internet  and direct people towards a client server based application architecture  the client server model provides financial barriers to entry to small publishers and individuals  and can be less efficient for sharing large files  as a reaction to this bandwidth throttling  several p p applications started implementing protocol obfuscation  such as the bittorrent protocol encryption  techniques for achieving  protocol obfuscation  involves removing otherwise easily identifiable properties of protocols  such as deterministic byte sequences and packet sizes  by making the data look as if it were random    the isp s solution to the high bandwidth is p p caching  where an isp stores the part of files most accessed by p p clients in order to save access to the internet     researchers have used computer simulations to aid in understanding and evaluating the complex behaviors of individuals within the network   networking research often relies on simulation in order to test and evaluate new ideas  an important requirement of this process is that results must be reproducible so that other researchers can replicate  validate  and extend existing work      if the research cannot be reproduced  then the opportunity for further research is hindered   even though new simulators continue to be released  the research community tends towards only a handful of open source simulators  the demand for features in simulators  as shown by our criteria and survey  is high  therefore  the community should work together to get these features in open source software  this would reduce the need for custom simulators  and hence increase repeatability and reputability of experiments      